{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 处理A股数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "new_data = pickle.load(open(r'/home/xuyang1/PatchTST/our_align_data.lm.V06.pickle/new_data.pkl', 'rb'))\n",
    "\n",
    "# data = pickle.load(open(r'/home/xuyang1/PatchTST/our_align_data.lm.V06.pickle/share.pkl', 'rb'))\n",
    "# ret = pickle.load(open(r'/home/xuyang1/PatchTST/ret.pkl', 'rb'))\n",
    "\n",
    "# ret = ret.reindex(data.index)\n",
    "# new_data = pd.concat([data.iloc(axis=1)[:-5], ret], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for col in tqdm(new_data.columns):\n",
    "#     tmp_df = pd.DataFrame({\"left\": new_data[col], \"right\": new_data.iloc(axis=1)[-5]})\n",
    "#     ic1 = tmp_df.groupby(\"datetime\").apply(lambda df: df[\"left\"].corr(df[\"right\"])).mean()\n",
    "#     tmp_df = pd.DataFrame({\"left\": new_data[col], \"right\": new_data.iloc(axis=1)[-3]})\n",
    "#     ic2 = tmp_df.groupby(\"datetime\").apply(lambda df: df[\"left\"].corr(df[\"right\"])).mean()\n",
    "#     if ic1 < 0 and ic2 < 0:\n",
    "#         new_data[col] *= -1\n",
    "#         print(col, ic1.mean(), ic2.mean())\n",
    "    # assert ic1 > 0 == ic2 > 0\n",
    "\n",
    "#     ic = df.groupby(\"datetime\").apply(lambda df: df[\"pred\"].corr(df[\"label\"]))\n",
    "\n",
    "\n",
    "# for index, col in enumerate(data.columns):\n",
    "#     print(index, col)\n",
    "# for index, col in enumerate(ret.columns):\n",
    "#     print(index, col)\n",
    "\n",
    "# for index in range(5):\n",
    "#     pred = data.iloc[:, -5 + index]\n",
    "#     label = ret.iloc[:, index]\n",
    "#     label = label.reindex(pred.index)\n",
    "#     print(pred.shape, label.shape)\n",
    "#     df = pd.DataFrame({\"pred\": pred, \"label\": label})\n",
    "#     ic = df.groupby(\"datetime\").apply(lambda df: df[\"pred\"].corr(df[\"label\"]))\n",
    "#     ric = df.groupby(\"datetime\").apply(lambda df: df[\"pred\"].corr(df[\"label\"], method=\"spearman\"))\n",
    "#     print(ic.mean(), ric.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "display(new_data.shape)\n",
    "display(new_data.head())\n",
    "# pickle.dump(new_data, open(r'/home/xuyang1/PatchTST/our_align_data.lm.V06.pickle/new_data.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandarallel import pandarallel\n",
    "pandarallel.initialize()\n",
    "\n",
    "tqdm.pandas()\n",
    "def calculate_quintile_ret(df):\n",
    "    label = df.iloc(axis=1)[-5]\n",
    "    quintile_ret = pd.Series(index=df.columns)\n",
    "    for col in df.columns:\n",
    "        tmp_df = pd.DataFrame({\"left\": df[col], \"right\": label}).sort_values(\"left\", ascending=False)\n",
    "        group1_ret = tmp_df.iloc[:int(len(tmp_df) * 0.2), 1].mean()\n",
    "        group5_ret = tmp_df.iloc[int(len(tmp_df) * 0.8):, 1].mean()\n",
    "        quintile_ret[col] = group1_ret - group5_ret\n",
    "\n",
    "    return quintile_ret\n",
    "\n",
    "# quintile_ret_data = new_data.groupby(\"datetime\").parallel_apply(calculate_quintile_ret)\n",
    "# pickle.dump(quintile_ret_data, open(r'/home/xuyang1/PatchTST/our_align_data.lm.V06.pickle/quintile_ret_data.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandarallel import pandarallel\n",
    "pandarallel.initialize()\n",
    "\n",
    "def calculate_top10p_ret(df):\n",
    "    label = df.iloc(axis=1)[-5]\n",
    "    top10p_ret = pd.Series(index=df.columns)\n",
    "    for col in df.columns:\n",
    "        tmp_df = pd.DataFrame({\"left\": df[col], \"right\": label}).sort_values(\"left\", ascending=False)\n",
    "        top10p_ret[col] = tmp_df.iloc[:int(len(tmp_df) * 0.1), 1].mean()\n",
    "    return top10p_ret\n",
    "\n",
    "# top10p_ret_data = new_data.groupby(\"datetime\").parallel_apply(calculate_top10p_ret)\n",
    "# pickle.dump(top10p_ret_data, open(r'/home/xuyang1/PatchTST/our_align_data.lm.V06.pickle/top10p_ret_data.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# top10p_ret_data = pickle.load(open(r'/home/xuyang1/PatchTST/our_align_data.lm.V06.pickle/top10p_ret_data.pkl', 'rb'))\n",
    "# top10p_ret_data.columns = [col[1] for col in top10p_ret_data.columns]\n",
    "# import plotly.express as px\n",
    "# top10p_ret_data_cumsum = top10p_ret_data.cumsum().iloc(axis=1)[::5]\n",
    "# fig = px.line(top10p_ret_data_cumsum, title=\"Top 10% cumulative return\")\n",
    "# fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quintile_ret_data = pickle.load(open(r'/home/xuyang1/PatchTST/our_align_data.lm.V06.pickle/quintile_ret_data.pkl', 'rb'))\n",
    "top10p_ret_data = pickle.load(open(r'/home/xuyang1/PatchTST/our_align_data.lm.V06.pickle/top10p_ret_data.pkl', 'rb'))\n",
    "\n",
    "quintile_ret_data_nolabel = quintile_ret_data.iloc(axis=1)[:-5]\n",
    "top10p_ret_data_nolabel = top10p_ret_data.iloc(axis=1)[:-5]\n",
    "quintile_ret_data_nolabel.columns = [col[1] for col in quintile_ret_data_nolabel.columns]\n",
    "top10p_ret_data_nolabel.columns = [col[1] for col in top10p_ret_data_nolabel.columns]\n",
    "quintile_ret_data_nolabel.index.names = [\"date\"]\n",
    "top10p_ret_data_nolabel.index.names = [\"date\"]\n",
    "\n",
    "quintile_ret_data_nolabel.to_csv(r'/home/xuyang1/PatchTST/PatchTST_supervised/dataset/quintile_stocka.csv')\n",
    "top10p_ret_data_nolabel.to_csv(r'/home/xuyang1/PatchTST/PatchTST_supervised/dataset/top10p_us.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "shape_series = pd.Series()\n",
    "import plotly.express as px\n",
    "for name in os.listdir(r'/home/xuyang1/PatchTST/PatchTST_supervised/dataset/'):\n",
    "    tmp = pd.read_csv(os.path.join(r'/home/xuyang1/PatchTST/PatchTST_supervised/dataset/', name))\n",
    "    shape_series[name] = tmp.shape\n",
    "    if \"stocka\" in name:\n",
    "        tmp = tmp.set_index(\"date\")\n",
    "        tmp = tmp.rolling(20).mean()\n",
    "        fig = px.line(tmp.iloc(axis=1)[::10], title=f\"{name} Top 10% cumulative return\")\n",
    "        fig.show()\n",
    "    \n",
    "# display(shape_series.to_frame(\"dataset_size\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 验证A股结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "label_quintile = pd.read_csv(r'/home/xuyang1/PatchTST/PatchTST_supervised/dataset/quintile_stocka.csv', index_col=0)\n",
    "# label_quintile = pd.DataFrame(pickle.load(open(r'/home/xuyang1/PatchTST/PatchTST_supervised/dataset/quintile_stocka_transformed.pkl', 'rb')), index=label_quintile.index, columns=label_quintile.columns)\n",
    "quintile_scaler = pickle.load(open(r'/home/xuyang1/PatchTST/PatchTST_supervised/dataset/quintile_stocka_scaler.pkl', 'rb'))\n",
    "label_top10p = pd.read_csv(r'/home/xuyang1/PatchTST/PatchTST_supervised/dataset/top10p_stocka.csv', index_col=0)\n",
    "# label_top10p = pd.DataFrame(pickle.load(open(r'/home/xuyang1/PatchTST/PatchTST_supervised/dataset/top10p_stocka_transformed.pkl', 'rb')), index=label_quintile.index, columns=label_quintile.columns)\n",
    "top10p_scaler = pickle.load(open(r'/home/xuyang1/PatchTST/PatchTST_supervised/dataset/top10p_stocka_scaler.pkl', 'rb'))\n",
    "losses = []\n",
    "for name in os.listdir(r\"/home/xuyang1/PatchTST/PatchTST_supervised/results\"):\n",
    "    model = name.split(\"_\")[4]\n",
    "    seq_length = name.split(\"_\")[3]\n",
    "    dataset = name.split(\"_\")[0]\n",
    "    if dataset == \"quintile\":\n",
    "        label = label_quintile\n",
    "    elif dataset == \"top10p\":\n",
    "        label = label_top10p\n",
    "    pred = np.load(rf\"/home/xuyang1/PatchTST/PatchTST_supervised/results/{name}/pred.npy\")\n",
    "\n",
    "    if dataset == \"quintile\":\n",
    "        scaler = quintile_scaler\n",
    "    elif dataset == \"top10p\":\n",
    "        scaler = top10p_scaler\n",
    "    pred = scaler.inverse_transform(np.reshape(pred, (-1, pred.shape[-1]))).reshape(pred.shape)\n",
    "\n",
    "    # if \"quinti\" not in dataset or seq_length != \"96\":\n",
    "    #     continue\n",
    "\n",
    "    test_length = pred.shape[0] + pred.shape[1] - 1\n",
    "    target_label = label.iloc[-test_length:,:]\n",
    "\n",
    "\n",
    "    # calculate pred to real label loss:\n",
    "    # loss = 0\n",
    "    # for i in range(pred.shape[0]):\n",
    "    #     pred_batch = pred[i]\n",
    "    #     label_batch = target_label.iloc[i:i+pred.shape[1],:].values\n",
    "    #     loss += np.mean((pred_batch - label_batch)**2)\n",
    "\n",
    "    # calculate real return with prediction:\n",
    "    # 1. use closest pred as real pread\n",
    "    real_pred = np.concatenate([pred[:-1,0,:], pred[-1,:,:]], axis=0)\n",
    "\n",
    "    # 2. use mean of pred as real pred\n",
    "    # pred_middata_np = np.zeros((pred.shape[0], pred.shape[0] + pred.shape[1] - 1, pred.shape[2]))\n",
    "    # pred_middata_np[:,:,:] = np.nan\n",
    "    # for i in range(pred.shape[0]):\n",
    "    #     pred_middata_np[i, i:i+pred.shape[1], :] = pred[i]\n",
    "    # real_pred = np.nanmean(pred_middata_np, axis=0, keepdims=False)\n",
    "\n",
    "\n",
    "    # 3. use mean of pred as real pred, but only use the first 20 pred\n",
    "    # real_pred = pred[:, :20, :].mean(axis=1, keepdims=False)\n",
    "\n",
    "    # pred_df = pd.DataFrame(real_pred, index=target_label.index[:real_pred.shape[0]], columns=target_label.columns)\n",
    "\n",
    "    # rolling_20_label = target_label.rolling(20).sum().shift(-20).iloc(axis=0)[:pred.shape[0]]\n",
    "\n",
    "    # average_ret = rolling_20_label.mean(axis=1).to_frame(\"average_ret\")\n",
    "\n",
    "    # rolling_20_pred_df = pred_df.rolling(20).mean().shift(-20).iloc(axis=0)[:-20]\n",
    "    # top50p_pred_ret = rolling_20_label[(pred_df.rank(axis=1, pct=True) > 0.5)].mean(axis=1).to_frame(\"top50p_pred_ret\")\n",
    "    # top20p_pred_ret = rolling_20_label[(pred_df.rank(axis=1, pct=True) > 0.8)].mean(axis=1).to_frame(\"top20p_pred_ret\")\n",
    "    # top10p_pred_ret = rolling_20_label[(pred_df.rank(axis=1, pct=True) > 0.9)].mean(axis=1).to_frame(\"top10p_pred_ret\")\n",
    "    # break\n",
    "\n",
    "    # show_df = pd.concat([average_ret, top50p_pred_ret, top20p_pred_ret, top10p_pred_ret], axis=1).cumsum(axis=0)\n",
    "    # show_df.plot()\n",
    "    # plt.title(f\"{dataset} {model} {seq_length}\")\n",
    "    # plt.show()\n",
    "\n",
    "    # pred_with_input = pd.concat([label.iloc[:-test_length,:], pred_df], axis=0)\n",
    "    # pred_with_input.columns = [col + \"_pred\" for col in pred_with_input.columns]\n",
    "\n",
    "    # fig, axes = plt.subplots(4, 4)\n",
    "    # fig.set_figheight(30)\n",
    "    # fig.set_figwidth(30)\n",
    "    # plt.subplots_adjust(wspace=None, hspace=None)\n",
    "    # axes = axes.reshape(4, 4)\n",
    "    # stride = label.shape[-1] // 16\n",
    "    # for i in range(16):\n",
    "    #     pred_and_gt = pd.concat([label.iloc(axis=1)[i*stride:i*stride+1], pred_with_input.iloc(axis=1)[i*stride:i*stride+1]], axis=1).rolling(20).mean()\n",
    "    #     pred_and_gt.plot(ax=axes[i//4, i%4], legend=True)\n",
    "    #     axes[i//4, i%4].set_title(label.columns[i*stride])\n",
    "    # plt.title(f\"{dataset} {model} {seq_length}\")\n",
    "    # plt.savefig(rf\"/home/xuyang1/PatchTST/PatchTST_supervised/plot_result/{dataset}_{model}_{seq_length}.png\")\n",
    "    # plt.show()\n",
    "    \n",
    "    # loss = ((real_pred - target_label)**2).mean(axis=0)\n",
    "    # losses.append(loss.to_frame(f\"{dataset}_{model}_{seq_length}\").T)\n",
    "    # loss = (np.quantile((real_pred - target_label).values.flatten(), 0.95))\n",
    "    # error = (real_pred - target_label).values.flatten()\n",
    "    # hist, bin_edges = np.histogram(error, bins=600, range=(-3, 3))\n",
    "    # if \"quintile\" in dataset and seq_length == \"96\":\n",
    "    #     plt.plot(bin_edges[:-1], hist, label=f\"{dataset} {model} {seq_length}\")\n",
    "    # rics = []\n",
    "    # for col in pred_df.columns:\n",
    "    #     rics.append(pred_df[col].corr(target_label[col], method=\"spearman\"))\n",
    "\n",
    "    # error = (real_pred - target_label).mean(axis=1).to_frame(f\"l1 loss {dataset} {seq_length} {model}\")\n",
    "    # losses.append(error)\n",
    "    # error = ((real_pred - target_label)**2).mean(axis=1).to_frame(f\"l2 loss {dataset} {seq_length} {model}\")\n",
    "    # losses.append(error)\n",
    "\n",
    "    # print(dataset, seq_length, model, sum(rics) / len(rics), sep=\",\")\n",
    "# loss_df = pd.concat(losses, axis=1)\n",
    "# print(loss_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "loss_df = loss_df.T.sort_index().T\n",
    "# display(loss_df.head())\n",
    "fig = px.line(loss_df.rolling(20).mean(), title=\"rolling 20 error\")\n",
    "fig.write_html(\"/home/xuyang1/PatchTST/PatchTST_supervised/plot_result/rolling_20_error.html\")\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 处理美股数据\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm \n",
    "from functools import partial\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "msn_data = pd.read_parquet(r\"/home/xuyang1/PatchTST/es_data/msn_data.parq\")\n",
    "price = pd.read_parquet(r\"/home/xuyang1/PatchTST/es_data/orig_price_info.parquet\")\n",
    "\n",
    "msn_data = msn_data.iloc(axis=1)[:-1]\n",
    "msn_data.columns = [i[1] for i in msn_data.columns]\n",
    "def guess_price(df, shift, horizon):\n",
    "    return df.price.shift(-shift-horizon) / df.price.shift(-shift) - 1\n",
    "\n",
    "tqdm.pandas()\n",
    "price[\"price_1d\"] = price.groupby(\"instrument\", group_keys=False).progress_apply(partial(guess_price, shift=1, horizon=1)).sort_index()\n",
    "new_data = msn_data.join(price[\"price_1d\"], how=\"left\").dropna(subset=['price_1d'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandarallel import pandarallel\n",
    "pandarallel.initialize()\n",
    "\n",
    "tqdm.pandas()\n",
    "def calculate_quintile_ret(df):\n",
    "    label = df.iloc(axis=1)[-1]\n",
    "    quintile_ret = pd.Series(index=df.columns)\n",
    "    for col in df.columns:\n",
    "        tmp_df = pd.DataFrame({\"left\": df[col], \"right\": label}).sort_values(\"left\", ascending=False)\n",
    "        group1_ret = tmp_df.iloc[:int(len(tmp_df) * 0.2), 1].mean()\n",
    "        group5_ret = tmp_df.iloc[int(len(tmp_df) * 0.8):, 1].mean()\n",
    "        quintile_ret[col] = group1_ret - group5_ret\n",
    "\n",
    "    return quintile_ret\n",
    "\n",
    "quintile_ret_data = new_data.groupby(\"datetime\").parallel_apply(calculate_quintile_ret)\n",
    "pickle.dump(quintile_ret_data, open(r'/home/xuyang1/PatchTST/es_data/quintile_ret_data.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandarallel import pandarallel\n",
    "pandarallel.initialize()\n",
    "\n",
    "def calculate_top10p_ret(df):\n",
    "    label = df.iloc(axis=1)[-1]\n",
    "    top10p_ret = pd.Series(index=df.columns)\n",
    "    for col in df.columns:\n",
    "        tmp_df = pd.DataFrame({\"left\": df[col], \"right\": label}).sort_values(\"left\", ascending=False)\n",
    "        top10p_ret[col] = tmp_df.iloc[:int(len(tmp_df) * 0.1), 1].mean()\n",
    "    return top10p_ret\n",
    "\n",
    "top10p_ret_data = new_data.groupby(\"datetime\").parallel_apply(calculate_top10p_ret)\n",
    "pickle.dump(top10p_ret_data, open(r'/home/xuyang1/PatchTST/es_data/top10p_ret_data.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quintile_ret_data = pickle.load(open(r'/home/xuyang1/PatchTST/es_data/quintile_ret_data.pkl', 'rb'))\n",
    "top10p_ret_data = pickle.load(open(r'/home/xuyang1/PatchTST/es_data/top10p_ret_data.pkl', 'rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in tqdm(quintile_ret_data.columns):\n",
    "    count = 0\n",
    "    while True:\n",
    "        upper = quintile_ret_data[col].mean() + quintile_ret_data[col].std() * 5\n",
    "        lower = quintile_ret_data[col].mean() - quintile_ret_data[col].std() * 5\n",
    "        if (upper > quintile_ret_data[col].max()) and (lower < quintile_ret_data[col].min()):\n",
    "            break\n",
    "        if count > 3:\n",
    "            break\n",
    "        count += 1\n",
    "        quintile_ret_data[col] = quintile_ret_data[col].clip(lower, upper,)\n",
    "\n",
    "for col in tqdm(top10p_ret_data.columns):\n",
    "    count = 0\n",
    "    while True:\n",
    "        upper = top10p_ret_data[col].mean() + top10p_ret_data[col].std() * 5\n",
    "        lower = top10p_ret_data[col].mean() - top10p_ret_data[col].std() * 5\n",
    "        if (upper > top10p_ret_data[col].max()) and (lower < top10p_ret_data[col].min()):\n",
    "            break\n",
    "        if count > 3:\n",
    "            break\n",
    "        count += 1\n",
    "        top10p_ret_data[col] = top10p_ret_data[col].clip(lower, upper,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quintile_ret_data = quintile_ret_data.iloc(axis=1)[:-1]\n",
    "top10p_ret_data = top10p_ret_data.iloc(axis=1)[:-1]\n",
    "\n",
    "quintile_ret_data.index.names = [\"date\"]\n",
    "top10p_ret_data.index.names = [\"date\"]\n",
    "\n",
    "quintile_ret_data.to_csv(r'/home/xuyang1/PatchTST/PatchTST_supervised/dataset/quintile_us.csv')\n",
    "top10p_ret_data.to_csv(r'/home/xuyang1/PatchTST/PatchTST_supervised/dataset/top10p_us.csv')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 验证美股结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "label_quintile = pd.read_csv(r'/home/xuyang1/PatchTST/PatchTST_supervised/dataset/quintile_us.csv', index_col=0)\n",
    "label_quintile = pd.DataFrame(pickle.load(open(r'/home/xuyang1/PatchTST/PatchTST_supervised/dataset/quintile_us_transformed.pkl', 'rb')), index=label_quintile.index, columns=label_quintile.columns)\n",
    "# quintile_scaler = pickle.load(open(r'/home/xuyang1/PatchTST/PatchTST_supervised/dataset/quintile_us_scaler.pkl', 'rb'))\n",
    "label_top10p = pd.read_csv(r'/home/xuyang1/PatchTST/PatchTST_supervised/dataset/top10p_us.csv', index_col=0)\n",
    "label_top10p = pd.DataFrame(pickle.load(open(r'/home/xuyang1/PatchTST/PatchTST_supervised/dataset/top10p_us_transformed.pkl', 'rb')), index=label_quintile.index, columns=label_quintile.columns)\n",
    "# top10p_scaler = pickle.load(open(r'/home/xuyang1/PatchTST/PatchTST_supervised/dataset/top10p_us_scaler.pkl', 'rb'))\n",
    "losses = []\n",
    "for name in os.listdir(r\"/home/xuyang1/PatchTST/PatchTST_supervised/results\"):\n",
    "    if \"stocka\" in name:\n",
    "        continue\n",
    "    model = name.split(\"_\")[4]\n",
    "    seq_length = name.split(\"_\")[3]\n",
    "    dataset = name.split(\"_\")[0]\n",
    "    if dataset == \"quintile\":\n",
    "        label = label_quintile\n",
    "    elif dataset == \"top10p\":\n",
    "        label = label_top10p\n",
    "    pred = np.load(rf\"/home/xuyang1/PatchTST/PatchTST_supervised/results/{name}/pred.npy\")\n",
    "\n",
    "    # if dataset == \"quintile\":\n",
    "    #     scaler = quintile_scaler\n",
    "    # elif dataset == \"top10p\":\n",
    "    #     scaler = top10p_scaler\n",
    "    # pred = scaler.inverse_transform(np.reshape(pred, (-1, pred.shape[-1]))).reshape(pred.shape)\n",
    "\n",
    "    # if \"quinti\" not in dataset or seq_length != \"96\":\n",
    "    #     continue\n",
    "\n",
    "    test_length = pred.shape[0] + pred.shape[1] - 1\n",
    "    target_label = label.iloc[-test_length:,:]\n",
    "\n",
    "\n",
    "    # calculate pred to real label loss:\n",
    "    # loss = 0\n",
    "    # for i in range(pred.shape[0]):\n",
    "    #     pred_batch = pred[i]\n",
    "    #     label_batch = target_label.iloc[i:i+pred.shape[1],:].values\n",
    "    #     loss += np.mean((pred_batch - label_batch)**2)\n",
    "\n",
    "    # calculate real return with prediction:\n",
    "    # 1. use closest pred as real pread\n",
    "    # real_pred = np.concatenate([pred[:-1,0,:], pred[-1,:,:]], axis=0)\n",
    "\n",
    "    # 2. use mean of pred as real pred\n",
    "    # pred_middata_np = np.zeros((pred.shape[0], pred.shape[0] + pred.shape[1] - 1, pred.shape[2]))\n",
    "    # pred_middata_np[:,:,:] = np.nan\n",
    "    # for i in range(pred.shape[0]):\n",
    "    #     pred_middata_np[i, i:i+pred.shape[1], :] = pred[i]\n",
    "    # real_pred = np.nanmean(pred_middata_np, axis=0, keepdims=False)\n",
    "\n",
    "\n",
    "    # 3. use mean of pred as real pred, but only use the first 20 pred\n",
    "    # real_pred = pred[:, :20, :].mean(axis=1, keepdims=False)\n",
    "\n",
    "    # 4. use farest pred as real pred\n",
    "    real_pred = np.concatenate([pred[0,:,:], pred[1:,-1,:]], axis=0)\n",
    "\n",
    "    pred_df = pd.DataFrame(real_pred, index=target_label.index[:real_pred.shape[0]], columns=target_label.columns)\n",
    "\n",
    "    # rolling_20_label = target_label.rolling(20).sum().shift(-20).iloc(axis=0)[:pred.shape[0]]\n",
    "\n",
    "    # average_ret = rolling_20_label.mean(axis=1).to_frame(\"average_ret\")\n",
    "\n",
    "    # rolling_20_pred_df = pred_df.rolling(20).mean().shift(-20).iloc(axis=0)[:-20]\n",
    "    # top50p_pred_ret = rolling_20_label[(pred_df.rank(axis=1, pct=True) > 0.5)].mean(axis=1).to_frame(\"top50p_pred_ret\")\n",
    "    # top20p_pred_ret = rolling_20_label[(pred_df.rank(axis=1, pct=True) > 0.8)].mean(axis=1).to_frame(\"top20p_pred_ret\")\n",
    "    # top10p_pred_ret = rolling_20_label[(pred_df.rank(axis=1, pct=True) > 0.9)].mean(axis=1).to_frame(\"top10p_pred_ret\")\n",
    "    # break\n",
    "\n",
    "    # show_df = pd.concat([average_ret, top50p_pred_ret, top20p_pred_ret, top10p_pred_ret], axis=1).cumsum(axis=0)\n",
    "    # show_df.plot()\n",
    "    # plt.title(f\"{dataset} {model} {seq_length}\")\n",
    "    # plt.show()\n",
    "\n",
    "    pred_with_input = pd.concat([label.iloc[:-test_length,:], pred_df], axis=0)\n",
    "    pred_with_input.columns = [col + \"_pred\" for col in pred_with_input.columns]\n",
    "\n",
    "    fig, axes = plt.subplots(5, 5)\n",
    "    fig.set_figheight(30)\n",
    "    fig.set_figwidth(30)\n",
    "    plt.subplots_adjust(wspace=None, hspace=None)\n",
    "    axes = axes.reshape(5, 5)\n",
    "    stride = label.shape[-1] // 25\n",
    "    for i in range(25):\n",
    "        pred_and_gt = pd.concat([label.iloc(axis=1)[i*stride:i*stride+1], pred_with_input.iloc(axis=1)[i*stride:i*stride+1]], axis=1).cumsum(axis=0)\n",
    "        pred_and_gt.plot(ax=axes[i//5, i%5], legend=True)\n",
    "        axes[i//5, i%5].set_title(label.columns[i*stride])\n",
    "    plt.title(f\"{dataset} {model} {seq_length}\")\n",
    "    plt.savefig(rf\"/home/xuyang1/PatchTST/PatchTST_supervised/plot_result/{dataset}_{model}_{seq_length}.png\")\n",
    "    # plt.show()\n",
    "    \n",
    "    # loss = ((real_pred - target_label)**2).mean(axis=0)\n",
    "    # losses.append(loss.to_frame(f\"{dataset}_{model}_{seq_length}\").T)\n",
    "    # loss = (np.quantile((real_pred - target_label).values.flatten(), 0.95))\n",
    "    # error = (real_pred - target_label).values.flatten()\n",
    "    # hist, bin_edges = np.histogram(error, bins=600, range=(-3, 3))\n",
    "    # if \"quintile\" in dataset and seq_length == \"96\":\n",
    "    #     plt.plot(bin_edges[:-1], hist, label=f\"{dataset} {model} {seq_length}\")\n",
    "    # rics = []\n",
    "    # for col in pred_df.columns:\n",
    "    #     rics.append(pred_df[col].corr(target_label[col], method=\"spearman\"))\n",
    "\n",
    "    # error = (real_pred - target_label).mean(axis=1).to_frame(f\"l1 loss {dataset} {seq_length} {model}\")\n",
    "    # losses.append(error)\n",
    "    # error = ((real_pred - target_label)**2).mean(axis=1).to_frame(f\"l2 loss {dataset} {seq_length} {model}\")\n",
    "    # losses.append(error)\n",
    "\n",
    "    # print(dataset, seq_length, model, sum(rics) / len(rics), sep=\",\")\n",
    "# loss_df = pd.concat(losses, axis=1)\n",
    "# print(loss_df.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "patchtst",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3e1fed312b402d124a26fd1f35e64e518612f0eec7447ca06c3f2decdc2741cd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
