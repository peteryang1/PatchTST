{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 处理A股数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "new_data = pickle.load(open(r'/home/xuyang1/PatchTST/our_align_data.lm.V06.pickle/new_data.pkl', 'rb'))\n",
    "\n",
    "# data = pickle.load(open(r'/home/xuyang1/PatchTST/our_align_data.lm.V06.pickle/share.pkl', 'rb'))\n",
    "# ret = pickle.load(open(r'/home/xuyang1/PatchTST/ret.pkl', 'rb'))\n",
    "\n",
    "# ret = ret.reindex(data.index)\n",
    "# new_data = pd.concat([data.iloc(axis=1)[:-5], ret], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for col in tqdm(new_data.columns):\n",
    "#     tmp_df = pd.DataFrame({\"left\": new_data[col], \"right\": new_data.iloc(axis=1)[-5]})\n",
    "#     ic1 = tmp_df.groupby(\"datetime\").apply(lambda df: df[\"left\"].corr(df[\"right\"])).mean()\n",
    "#     tmp_df = pd.DataFrame({\"left\": new_data[col], \"right\": new_data.iloc(axis=1)[-3]})\n",
    "#     ic2 = tmp_df.groupby(\"datetime\").apply(lambda df: df[\"left\"].corr(df[\"right\"])).mean()\n",
    "#     if ic1 < 0 and ic2 < 0:\n",
    "#         new_data[col] *= -1\n",
    "#         print(col, ic1.mean(), ic2.mean())\n",
    "    # assert ic1 > 0 == ic2 > 0\n",
    "\n",
    "#     ic = df.groupby(\"datetime\").apply(lambda df: df[\"pred\"].corr(df[\"label\"]))\n",
    "\n",
    "\n",
    "# for index, col in enumerate(data.columns):\n",
    "#     print(index, col)\n",
    "# for index, col in enumerate(ret.columns):\n",
    "#     print(index, col)\n",
    "\n",
    "# for index in range(5):\n",
    "#     pred = data.iloc[:, -5 + index]\n",
    "#     label = ret.iloc[:, index]\n",
    "#     label = label.reindex(pred.index)\n",
    "#     print(pred.shape, label.shape)\n",
    "#     df = pd.DataFrame({\"pred\": pred, \"label\": label})\n",
    "#     ic = df.groupby(\"datetime\").apply(lambda df: df[\"pred\"].corr(df[\"label\"]))\n",
    "#     ric = df.groupby(\"datetime\").apply(lambda df: df[\"pred\"].corr(df[\"label\"], method=\"spearman\"))\n",
    "#     print(ic.mean(), ric.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "display(new_data.shape)\n",
    "display(new_data.head())\n",
    "# pickle.dump(new_data, open(r'/home/xuyang1/PatchTST/our_align_data.lm.V06.pickle/new_data.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandarallel import pandarallel\n",
    "pandarallel.initialize()\n",
    "\n",
    "tqdm.pandas()\n",
    "def calculate_quintile_ret(df):\n",
    "    label = df.iloc(axis=1)[-5]\n",
    "    quintile_ret = pd.Series(index=df.columns)\n",
    "    for col in df.columns:\n",
    "        tmp_df = pd.DataFrame({\"left\": df[col], \"right\": label}).sort_values(\"left\", ascending=False)\n",
    "        group1_ret = tmp_df.iloc[:int(len(tmp_df) * 0.2), 1].mean()\n",
    "        group5_ret = tmp_df.iloc[int(len(tmp_df) * 0.8):, 1].mean()\n",
    "        quintile_ret[col] = group1_ret - group5_ret\n",
    "\n",
    "    return quintile_ret\n",
    "\n",
    "# quintile_ret_data = new_data.groupby(\"datetime\").parallel_apply(calculate_quintile_ret)\n",
    "# pickle.dump(quintile_ret_data, open(r'/home/xuyang1/PatchTST/our_align_data.lm.V06.pickle/quintile_ret_data.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandarallel import pandarallel\n",
    "pandarallel.initialize()\n",
    "\n",
    "def calculate_top10p_ret(df):\n",
    "    label = df.iloc(axis=1)[-5]\n",
    "    top10p_ret = pd.Series(index=df.columns)\n",
    "    for col in df.columns:\n",
    "        tmp_df = pd.DataFrame({\"left\": df[col], \"right\": label}).sort_values(\"left\", ascending=False)\n",
    "        top10p_ret[col] = tmp_df.iloc[:int(len(tmp_df) * 0.1), 1].mean()\n",
    "    return top10p_ret\n",
    "\n",
    "# top10p_ret_data = new_data.groupby(\"datetime\").parallel_apply(calculate_top10p_ret)\n",
    "# pickle.dump(top10p_ret_data, open(r'/home/xuyang1/PatchTST/our_align_data.lm.V06.pickle/top10p_ret_data.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# top10p_ret_data = pickle.load(open(r'/home/xuyang1/PatchTST/our_align_data.lm.V06.pickle/top10p_ret_data.pkl', 'rb'))\n",
    "# top10p_ret_data.columns = [col[1] for col in top10p_ret_data.columns]\n",
    "# import plotly.express as px\n",
    "# top10p_ret_data_cumsum = top10p_ret_data.cumsum().iloc(axis=1)[::5]\n",
    "# fig = px.line(top10p_ret_data_cumsum, title=\"Top 10% cumulative return\")\n",
    "# fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quintile_ret_data = pickle.load(open(r'/home/xuyang1/PatchTST/our_align_data.lm.V06.pickle/quintile_ret_data.pkl', 'rb'))\n",
    "top10p_ret_data = pickle.load(open(r'/home/xuyang1/PatchTST/our_align_data.lm.V06.pickle/top10p_ret_data.pkl', 'rb'))\n",
    "\n",
    "quintile_ret_data_nolabel = quintile_ret_data.iloc(axis=1)[:-5]\n",
    "top10p_ret_data_nolabel = top10p_ret_data.iloc(axis=1)[:-5]\n",
    "quintile_ret_data_nolabel.columns = [col[1] for col in quintile_ret_data_nolabel.columns]\n",
    "top10p_ret_data_nolabel.columns = [col[1] for col in top10p_ret_data_nolabel.columns]\n",
    "quintile_ret_data_nolabel.index.names = [\"date\"]\n",
    "top10p_ret_data_nolabel.index.names = [\"date\"]\n",
    "\n",
    "quintile_ret_data_nolabel.to_csv(r'/home/xuyang1/PatchTST/PatchTST_supervised/dataset/quintile_stocka.csv')\n",
    "top10p_ret_data_nolabel.to_csv(r'/home/xuyang1/PatchTST/PatchTST_supervised/dataset/top10p_us.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "shape_series = pd.Series()\n",
    "import plotly.express as px\n",
    "for name in os.listdir(r'/home/xuyang1/PatchTST/PatchTST_supervised/dataset/'):\n",
    "    tmp = pd.read_csv(os.path.join(r'/home/xuyang1/PatchTST/PatchTST_supervised/dataset/', name))\n",
    "    shape_series[name] = tmp.shape\n",
    "    if \"stocka\" in name:\n",
    "        tmp = tmp.set_index(\"date\")\n",
    "        tmp = tmp.rolling(20).mean()\n",
    "        fig = px.line(tmp.iloc(axis=1)[::10], title=f\"{name} Top 10% cumulative return\")\n",
    "        fig.show()\n",
    "    \n",
    "# display(shape_series.to_frame(\"dataset_size\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 验证结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "label_stocka_quintile = pd.read_csv(r'/home/xuyang1/PatchTST/PatchTST_supervised/dataset/quintile_stocka.csv', index_col=0)\n",
    "label_us_quintile = pd.read_csv(r'/home/xuyang1/PatchTST/PatchTST_supervised/dataset/quintile_us.csv', index_col=0)\n",
    "# label_stocka_quintile = pd.DataFrame(pickle.load(open(r'/home/xuyang1/PatchTST/PatchTST_supervised/dataset/quintile_stocka_transformed.pkl', 'rb')), index=label_stocka_quintile.index, columns=label_stocka_quintile.columns)\n",
    "# label_us_quintile = pd.DataFrame(pickle.load(open(r'/home/xuyang1/PatchTST/PatchTST_supervised/dataset/quintile_us_transformed.pkl', 'rb')), index=label_us_quintile.index, columns=label_us_quintile.columns)\n",
    "label_stocka_top10p = pd.read_csv(r'/home/xuyang1/PatchTST/PatchTST_supervised/dataset/top10p_stocka.csv', index_col=0)\n",
    "label_us_top10p = pd.read_csv(r'/home/xuyang1/PatchTST/PatchTST_supervised/dataset/top10p_us.csv', index_col=0)\n",
    "# label_stocka_top10p = pd.DataFrame(pickle.load(open(r'/home/xuyang1/PatchTST/PatchTST_supervised/dataset/top10p_stocka_transformed.pkl', 'rb')), index=label_stocka_top10p.index, columns=label_stocka_top10p.columns)\n",
    "# label_us_top10p = pd.DataFrame(pickle.load(open(r'/home/xuyang1/PatchTST/PatchTST_supervised/dataset/top10p_us_transformed.pkl', 'rb')), index=label_us_top10p.index, columns=label_us_top10p.columns)\n",
    "\n",
    "quintile_stocka_scaler = pickle.load(open(r'/home/xuyang1/PatchTST/PatchTST_supervised/dataset/quintile_stocka_scaler.pkl', 'rb'))\n",
    "top10p_stocka_scaler = pickle.load(open(r'/home/xuyang1/PatchTST/PatchTST_supervised/dataset/top10p_stocka_scaler.pkl', 'rb'))\n",
    "quintile_us_scaler = pickle.load(open(r'/home/xuyang1/PatchTST/PatchTST_supervised/dataset/quintile_us_scaler.pkl', 'rb'))\n",
    "top10p_us_scaler = pickle.load(open(r'/home/xuyang1/PatchTST/PatchTST_supervised/dataset/top10p_us_scaler.pkl', 'rb'))\n",
    "losses = []\n",
    "for name in os.listdir(r\"/home/xuyang1/PatchTST/PatchTST_supervised/results\"):\n",
    "    model = name.split(\"_\")[4]\n",
    "    seq_length = name.split(\"_\")[3]\n",
    "    dataset = name.split(\"_\")[0]\n",
    "    dataset_country = name.split(\"_\")[1]\n",
    "    if dataset == \"quintile\" and dataset_country == \"stocka\":\n",
    "        label = label_stocka_quintile\n",
    "    elif dataset == \"quintile\" and dataset_country == \"us\":\n",
    "        label = label_us_quintile\n",
    "    elif dataset == \"top10p\" and dataset_country == \"stocka\":\n",
    "        label = label_stocka_top10p\n",
    "    elif dataset == \"top10p\" and dataset_country == \"us\":\n",
    "        label = label_us_top10p\n",
    "    pred = np.load(rf\"/home/xuyang1/PatchTST/PatchTST_supervised/results/{name}/pred.npy\")\n",
    "\n",
    "    if dataset == \"quintile\" and dataset_country == \"stocka\":\n",
    "        scaler = quintile_stocka_scaler\n",
    "    elif dataset == \"quintile\" and dataset_country == \"us\":\n",
    "        scaler = quintile_us_scaler\n",
    "    elif dataset == \"top10p\" and dataset_country == \"stocka\":\n",
    "        scaler = top10p_stocka_scaler\n",
    "    elif dataset == \"top10p\" and dataset_country == \"us\":\n",
    "        scaler = top10p_us_scaler\n",
    "    pred = scaler.inverse_transform(np.reshape(pred, (-1, pred.shape[-1]))).reshape(pred.shape)\n",
    "\n",
    "    # if \"quinti\" not in dataset or seq_length != \"96\":\n",
    "    #     continue\n",
    "\n",
    "    test_length = int(len(label) * 0.2)\n",
    "\n",
    "    for i in range(pred.shape[-1]):\n",
    "        if label.values[:-test_length,i].mean() < 0:\n",
    "            label.iloc[:,i] = label.iloc[:,i] * -1\n",
    "            pred[:,:,i] = -pred[:,:,i]\n",
    "    target_label = label.iloc[-test_length:,:]\n",
    "    if pred.shape[0] > target_label.shape[0]:\n",
    "        pred = pred[:target_label.shape[0],:,:]\n",
    "    elif pred.shape[0] < target_label.shape[0]:\n",
    "        target_label = target_label[:pred.shape[0],:]\n",
    "\n",
    "\n",
    "    # calculate pred to real label loss:\n",
    "    # loss = 0\n",
    "    # for i in range(pred.shape[0]):\n",
    "    #     pred_batch = pred[i]\n",
    "    #     label_batch = target_label.iloc[i:i+pred.shape[1],:].values\n",
    "    #     loss += np.mean((pred_batch - label_batch)**2)\n",
    "\n",
    "    # calculate real return with prediction:\n",
    "    # 1. use closest pred as real pread\n",
    "    # real_pred = pred[:,0,:]\n",
    "\n",
    "    # 2. use mean of pred as real pred\n",
    "    # pred_middata_np = np.zeros((pred.shape[0], pred.shape[0] + pred.shape[1] - 1, pred.shape[2]))\n",
    "    # pred_middata_np[:,:,:] = np.nan\n",
    "    # for i in range(pred.shape[0]):\n",
    "    #     pred_middata_np[i, i:i+pred.shape[1], :] = pred[i]\n",
    "    # real_pred = np.nanmean(pred_middata_np, axis=0, keepdims=False)\n",
    "\n",
    "    # 3. use mean of pred as real pred, but only use the first 20 pred\n",
    "    real_pred = pred[:, :20, :].mean(axis=1, keepdims=False)\n",
    "\n",
    "    pred_df = pd.DataFrame(real_pred, index=target_label.index[:real_pred.shape[0]], columns=target_label.columns)\n",
    "\n",
    "    # ---- calculate rolling 20 days return ----\n",
    "    rolling_20_label = target_label.rolling(20).mean().shift(-19).iloc(axis=0)[:-19]\n",
    "    average_ret = rolling_20_label.mean(axis=1).to_frame(\"average_ret\")\n",
    "\n",
    "    past_rolling_20_label = target_label.rolling(20, closed=\"left\", min_periods=1).mean()\n",
    "\n",
    "    follow_past_20days_50p = rolling_20_label[(past_rolling_20_label.rank(axis=1, pct=True) > 0.5)].mean(axis=1).to_frame(\"follow_past_20days_50p\")\n",
    "    follow_past_20days_20p = rolling_20_label[(past_rolling_20_label.rank(axis=1, pct=True) > 0.8)].mean(axis=1).to_frame(\"follow_past_20days_20p\")\n",
    "    follow_past_20days_10p = rolling_20_label[(past_rolling_20_label.rank(axis=1, pct=True) > 0.9)].mean(axis=1).to_frame(\"follow_past_20days_10p\")\n",
    "\n",
    "    rolling_20_pred_df = pred_df.rolling(20).mean().shift(-20).iloc(axis=0)[:-20]\n",
    "    top50p_pred_ret = rolling_20_label[(pred_df.rank(axis=1, pct=True) > 0.5)].mean(axis=1).to_frame(\"top50p_pred_ret\")\n",
    "    top20p_pred_ret = rolling_20_label[(pred_df.rank(axis=1, pct=True) > 0.8)].mean(axis=1).to_frame(\"top20p_pred_ret\")\n",
    "    top10p_pred_ret = rolling_20_label[(pred_df.rank(axis=1, pct=True) > 0.9)].mean(axis=1).to_frame(\"top10p_pred_ret\")\n",
    "    show_df = pd.concat([average_ret, top50p_pred_ret, top20p_pred_ret, top10p_pred_ret, follow_past_20days_50p, follow_past_20days_20p, follow_past_20days_10p], axis=1).cumsum(axis=0)\n",
    "    show_df.plot()\n",
    "    plt.title(f\"{dataset_country} {dataset} {model} {seq_length}\")\n",
    "    # plt.show()\n",
    "\n",
    "    # ---- plot pred and label in same photo ----\n",
    "    # pred_with_input = pd.concat([label.iloc[:-test_length,:], pred_df], axis=0)\n",
    "    # pred_with_input.columns = [col + \"_pred\" for col in pred_with_input.columns]\n",
    "    # fig, axes = plt.subplots(5, 5)\n",
    "    # fig.set_figheight(30)\n",
    "    # fig.set_figwidth(30)\n",
    "    # plt.subplots_adjust(wspace=None, hspace=None)\n",
    "    # axes = axes.reshape(5, 5)\n",
    "    # stride = label.shape[-1] // 25\n",
    "    # for i in range(25):\n",
    "    #     pred_and_gt = pd.concat([label.iloc(axis=1)[i*stride:i*stride+1], pred_with_input.iloc(axis=1)[i*stride:i*stride+1]], axis=1).cumsum(axis=0)\n",
    "    #     pred_and_gt.plot(ax=axes[i//5, i%5], legend=True)\n",
    "    #     axes[i//5, i%5].set_title(label.columns[i*stride])\n",
    "    # plt.title(f\"{dataset_country} {dataset} {model} {seq_length}\")\n",
    "    plt.savefig(rf\"/home/xuyang1/PatchTST/PatchTST_supervised/plot_result/{dataset_country}_{dataset}_{model}_{seq_length}.png\")\n",
    "    # plt.show()\n",
    "    \n",
    "    # rics = []\n",
    "    # for col in pred_df.columns:\n",
    "    #     rics.append(pred_df[col].corr(target_label[col], method=\"spearman\"))\n",
    "\n",
    "    # print(dataset_country, dataset, seq_length, model, sum(rics) / len(rics), sep=\",\")\n",
    "    plt.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 处理美股数据\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm \n",
    "from functools import partial\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "msn_data = pd.read_parquet(r\"/home/xuyang1/PatchTST/es_data/msn_data.parq\")\n",
    "price = pd.read_parquet(r\"/home/xuyang1/PatchTST/es_data/orig_price_info.parquet\")\n",
    "\n",
    "msn_data = msn_data.iloc(axis=1)[:-1]\n",
    "msn_data.columns = [i[1] for i in msn_data.columns]\n",
    "def guess_price(df, shift, horizon):\n",
    "    return df.price.shift(-shift-horizon) / df.price.shift(-shift) - 1\n",
    "\n",
    "tqdm.pandas()\n",
    "price[\"price_1d\"] = price.groupby(\"instrument\", group_keys=False).progress_apply(partial(guess_price, shift=1, horizon=1)).sort_index()\n",
    "new_data = msn_data.join(price[\"price_1d\"], how=\"left\").dropna(subset=['price_1d'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandarallel import pandarallel\n",
    "pandarallel.initialize()\n",
    "\n",
    "tqdm.pandas()\n",
    "def calculate_quintile_ret(df):\n",
    "    label = df.iloc(axis=1)[-1]\n",
    "    quintile_ret = pd.Series(index=df.columns)\n",
    "    for col in df.columns:\n",
    "        tmp_df = pd.DataFrame({\"left\": df[col], \"right\": label}).sort_values(\"left\", ascending=False)\n",
    "        group1_ret = tmp_df.iloc[:int(len(tmp_df) * 0.2), 1].mean()\n",
    "        group5_ret = tmp_df.iloc[int(len(tmp_df) * 0.8):, 1].mean()\n",
    "        quintile_ret[col] = group1_ret - group5_ret\n",
    "\n",
    "    return quintile_ret\n",
    "\n",
    "quintile_ret_data = new_data.groupby(\"datetime\").parallel_apply(calculate_quintile_ret)\n",
    "pickle.dump(quintile_ret_data, open(r'/home/xuyang1/PatchTST/es_data/quintile_ret_data.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandarallel import pandarallel\n",
    "pandarallel.initialize()\n",
    "\n",
    "def calculate_top10p_ret(df):\n",
    "    label = df.iloc(axis=1)[-1]\n",
    "    top10p_ret = pd.Series(index=df.columns)\n",
    "    for col in df.columns:\n",
    "        tmp_df = pd.DataFrame({\"left\": df[col], \"right\": label}).sort_values(\"left\", ascending=False)\n",
    "        top10p_ret[col] = tmp_df.iloc[:int(len(tmp_df) * 0.1), 1].mean()\n",
    "    return top10p_ret\n",
    "\n",
    "top10p_ret_data = new_data.groupby(\"datetime\").parallel_apply(calculate_top10p_ret)\n",
    "pickle.dump(top10p_ret_data, open(r'/home/xuyang1/PatchTST/es_data/top10p_ret_data.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quintile_ret_data = pickle.load(open(r'/home/xuyang1/PatchTST/es_data/quintile_ret_data.pkl', 'rb'))\n",
    "top10p_ret_data = pickle.load(open(r'/home/xuyang1/PatchTST/es_data/top10p_ret_data.pkl', 'rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in tqdm(quintile_ret_data.columns):\n",
    "    count = 0\n",
    "    while True:\n",
    "        upper = quintile_ret_data[col].mean() + quintile_ret_data[col].std() * 5\n",
    "        lower = quintile_ret_data[col].mean() - quintile_ret_data[col].std() * 5\n",
    "        if (upper > quintile_ret_data[col].max()) and (lower < quintile_ret_data[col].min()):\n",
    "            break\n",
    "        if count > 3:\n",
    "            break\n",
    "        count += 1\n",
    "        quintile_ret_data[col] = quintile_ret_data[col].clip(lower, upper,)\n",
    "\n",
    "for col in tqdm(top10p_ret_data.columns):\n",
    "    count = 0\n",
    "    while True:\n",
    "        upper = top10p_ret_data[col].mean() + top10p_ret_data[col].std() * 5\n",
    "        lower = top10p_ret_data[col].mean() - top10p_ret_data[col].std() * 5\n",
    "        if (upper > top10p_ret_data[col].max()) and (lower < top10p_ret_data[col].min()):\n",
    "            break\n",
    "        if count > 3:\n",
    "            break\n",
    "        count += 1\n",
    "        top10p_ret_data[col] = top10p_ret_data[col].clip(lower, upper,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quintile_ret_data = quintile_ret_data.iloc(axis=1)[:-1]\n",
    "top10p_ret_data = top10p_ret_data.iloc(axis=1)[:-1]\n",
    "\n",
    "quintile_ret_data.index.names = [\"date\"]\n",
    "top10p_ret_data.index.names = [\"date\"]\n",
    "\n",
    "quintile_ret_data.to_csv(r'/home/xuyang1/PatchTST/PatchTST_supervised/dataset/quintile_us.csv')\n",
    "top10p_ret_data.to_csv(r'/home/xuyang1/PatchTST/PatchTST_supervised/dataset/top10p_us.csv')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 记录所有loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "lines = open(r\"/home/xuyang1/PatchTST/final_result.txt\").readlines()\n",
    "\n",
    "result_df = pd.DataFrame(columns=[\"dataset\", \"country\", \"seq_length\", \"model\", \"mse\", \"mae\", \"rse\"])\n",
    "for line in lines:\n",
    "    line = line.strip()\n",
    "    if len(line) == 0:\n",
    "        continue\n",
    "    dataset = line.split(\"_\")[0]\n",
    "    dataset_country = line.split(\"_\")[1]\n",
    "    seq_len = int(line.split(\"_\")[3])\n",
    "    model = line.split(\"_\")[4]\n",
    "    mse = float(line.split(\"mse:\")[1].split(\",\")[0])\n",
    "    mae = float(line.split(\"mae:\")[1].split(\",\")[0])\n",
    "    rse = float(line.split(\"rse:\")[1].split(\",\")[0])\n",
    "    result_df = pd.concat([result_df, pd.DataFrame([[dataset, dataset_country, seq_len, model, mse, mae, rse]], columns=result_df.columns)], axis=0)\n",
    "\n",
    "result_df = result_df.set_index([\"dataset\", \"country\", \"seq_length\", \"model\"]).sort_index()\n",
    "result_df.to_excel(r\"/home/xuyang1/PatchTST/final_result.xlsx\")\n",
    "# display(result_df)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 拼接所有图片"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "file_names = os.listdir(r\"/home/xuyang1/PatchTST/PatchTST_supervised/plot_result\")\n",
    "\n",
    "result_df = pd.DataFrame(columns=[\"dataset\", \"country\", \"seq_length\", \"model\", \"name\"])\n",
    "for original_name in file_names:\n",
    "    name = original_name.split(\".\")[0]\n",
    "    dataset_country = name.split(\"_\")[0]\n",
    "    dataset = name.split(\"_\")[1]\n",
    "    model = name.split(\"_\")[2]\n",
    "    seq_len = int(name.split(\"_\")[3])\n",
    "    \n",
    "    result_df = pd.concat([result_df, pd.DataFrame([[dataset, dataset_country, seq_len, model, f\"/home/xuyang1/PatchTST/PatchTST_supervised/plot_result/{original_name}\"]], columns=result_df.columns)], axis=0)\n",
    "\n",
    "\n",
    "\n",
    "result_df = result_df.set_index([\"dataset\", \"country\", \"model\", \"seq_length\", ]).sort_index()\n",
    "# result_df = result_df.set_index([\"model\"])\n",
    "for dataset in result_df.index.get_level_values(\"dataset\").drop_duplicates():\n",
    "    part_df1 = result_df.loc[dataset]\n",
    "    for country in part_df1.index.get_level_values(\"country\").drop_duplicates():\n",
    "        part_df2 = part_df1.loc[country]\n",
    "        images = []\n",
    "        for model in part_df2.index.get_level_values(\"model\").drop_duplicates():\n",
    "            part_df3 = part_df2.loc[model]\n",
    "            for seq_length in part_df3.index.get_level_values(\"seq_length\").drop_duplicates():\n",
    "                part_df4 = part_df3.loc(axis=0)[seq_length]\n",
    "                img = cv2.imread(part_df4[\"name\"])\n",
    "                cv2.putText(img, f\"{dataset}_{country}_{seq_length}_{model}\", (0, 100), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "                images.append(img)\n",
    "        height, width, _ = images[0].shape  \n",
    "        row_num = len(images) // 4\n",
    "        col_num = 4\n",
    "    \n",
    "        # 创建一个空白图片，用于存放拼接后的图片  \n",
    "        result_image = np.zeros((height * row_num, width * col_num, 3), np.uint8)  \n",
    "    \n",
    "        # 遍历每个图片，按照九宫格的顺序将其粘贴到结果图片上  \n",
    "        for i, image in enumerate(images):  \n",
    "            x = i % col_num * width  \n",
    "            y = i // col_num * height  \n",
    "            result_image[y:y+height, x:x+width] = image\n",
    "        result_image = cv2.resize(result_image, (result_image.shape[1], result_image.shape[0]))\n",
    "        cv2.imwrite(f'/home/xuyang1/PatchTST/PatchTST_supervised/plot_result_combined/{dataset}_{country}.png', result_image,)\n",
    "\n",
    "\n",
    "\n",
    "display(result_df)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 对比 self supervised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "label_stocka_quintile = pd.read_csv(r'/home/xuyang1/PatchTST/PatchTST_supervised/dataset/quintile_stocka.csv', index_col=0)\n",
    "label_us_quintile = pd.read_csv(r'/home/xuyang1/PatchTST/PatchTST_supervised/dataset/quintile_us.csv', index_col=0)\n",
    "# label_stocka_quintile = pd.DataFrame(pickle.load(open(r'/home/xuyang1/PatchTST/PatchTST_supervised/dataset/quintile_stocka_transformed.pkl', 'rb')), index=label_stocka_quintile.index, columns=label_stocka_quintile.columns)\n",
    "# label_us_quintile = pd.DataFrame(pickle.load(open(r'/home/xuyang1/PatchTST/PatchTST_supervised/dataset/quintile_us_transformed.pkl', 'rb')), index=label_us_quintile.index, columns=label_us_quintile.columns)\n",
    "label_stocka_top10p = pd.read_csv(r'/home/xuyang1/PatchTST/PatchTST_supervised/dataset/top10p_stocka.csv', index_col=0)\n",
    "label_us_top10p = pd.read_csv(r'/home/xuyang1/PatchTST/PatchTST_supervised/dataset/top10p_us.csv', index_col=0)\n",
    "# label_stocka_top10p = pd.DataFrame(pickle.load(open(r'/home/xuyang1/PatchTST/PatchTST_supervised/dataset/top10p_stocka_transformed.pkl', 'rb')), index=label_stocka_top10p.index, columns=label_stocka_top10p.columns)\n",
    "# label_us_top10p = pd.DataFrame(pickle.load(open(r'/home/xuyang1/PatchTST/PatchTST_supervised/dataset/top10p_us_transformed.pkl', 'rb')), index=label_us_top10p.index, columns=label_us_top10p.columns)\n",
    "\n",
    "quintile_stocka_scaler = pickle.load(open(r'/home/xuyang1/PatchTST/PatchTST_supervised/dataset/quintile_stocka_scaler.pkl', 'rb'))\n",
    "top10p_stocka_scaler = pickle.load(open(r'/home/xuyang1/PatchTST/PatchTST_supervised/dataset/top10p_stocka_scaler.pkl', 'rb'))\n",
    "quintile_us_scaler = pickle.load(open(r'/home/xuyang1/PatchTST/PatchTST_supervised/dataset/quintile_us_scaler.pkl', 'rb'))\n",
    "top10p_us_scaler = pickle.load(open(r'/home/xuyang1/PatchTST/PatchTST_supervised/dataset/top10p_us_scaler.pkl', 'rb'))\n",
    "losses = []\n",
    "for name in os.listdir(r\"/home/xuyang1/PatchTST/PatchTST_self_supervised/results\"):\n",
    "    if \"finetune\" in name or \"linear-probe\" in name:\n",
    "        model = \"PatchTST\" + name.split(\"_\")[3]\n",
    "        seq_length = name.split(\"tw\")[-1].split(\"_\")[0]\n",
    "        dataset = name.split(\"_\")[0]\n",
    "        dataset_country = name.split(\"_\")[1]\n",
    "    else:\n",
    "        model = name.split(\"_\")[4]\n",
    "        seq_length = name.split(\"_\")[3]\n",
    "        dataset = name.split(\"_\")[0]\n",
    "        dataset_country = name.split(\"_\")[1]\n",
    "    if dataset == \"quintile\" and dataset_country == \"stocka\":\n",
    "        label = label_stocka_quintile\n",
    "    elif dataset == \"quintile\" and dataset_country == \"us\":\n",
    "        label = label_us_quintile\n",
    "    elif dataset == \"top10p\" and dataset_country == \"stocka\":\n",
    "        label = label_stocka_top10p\n",
    "    elif dataset == \"top10p\" and dataset_country == \"us\":\n",
    "        label = label_us_top10p\n",
    "    pred = np.load(rf\"/home/xuyang1/PatchTST/PatchTST_self_supervised/results/{name}/pred.npy\")\n",
    "    if \"stocka\" == dataset_country:\n",
    "        pred = pred[:593, :, :]\n",
    "    else:\n",
    "        pred = pred[:986, :, :]\n",
    "\n",
    "    if dataset == \"quintile\" and dataset_country == \"stocka\":\n",
    "        scaler = quintile_stocka_scaler\n",
    "    elif dataset == \"quintile\" and dataset_country == \"us\":\n",
    "        scaler = quintile_us_scaler\n",
    "    elif dataset == \"top10p\" and dataset_country == \"stocka\":\n",
    "        scaler = top10p_stocka_scaler\n",
    "    elif dataset == \"top10p\" and dataset_country == \"us\":\n",
    "        scaler = top10p_us_scaler\n",
    "    pred = scaler.inverse_transform(np.reshape(pred, (-1, pred.shape[-1]))).reshape(pred.shape)\n",
    "\n",
    "    # if \"quinti\" not in dataset or seq_length != \"96\":\n",
    "    #     continue\n",
    "\n",
    "    test_length = int(len(label) * 0.2)\n",
    "\n",
    "    for i in range(pred.shape[-1]):\n",
    "        if label.values[:-test_length,i].mean() < 0:\n",
    "            label.iloc[:,i] = label.iloc[:,i] * -1\n",
    "            pred[:,:,i] = -pred[:,:,i]\n",
    "    target_label = label.iloc[-test_length:,:]\n",
    "    if pred.shape[0] > target_label.shape[0]:\n",
    "        pred = pred[:target_label.shape[0],:,:]\n",
    "    elif pred.shape[0] < target_label.shape[0]:\n",
    "        target_label = target_label.iloc[:pred.shape[0],:]\n",
    "\n",
    "\n",
    "    # calculate pred to real label loss:\n",
    "    # loss = 0\n",
    "    # for i in range(pred.shape[0]):\n",
    "    #     pred_batch = pred[i]\n",
    "    #     label_batch = target_label.iloc[i:i+pred.shape[1],:].values\n",
    "    #     loss += np.mean((pred_batch - label_batch)**2)\n",
    "    # mse = np.mean((pred[:,0,:] - target_label.values)**2)\n",
    "    # mae = np.mean(np.abs(pred[:,0,:] - target_label.values))\n",
    "    # print(f\"{model},{dataset},{dataset_country},{seq_length},{mse},{mae}\")\n",
    "    # continue\n",
    "\n",
    "    # calculate real return with prediction:\n",
    "    # 1. use closest pred as real pread\n",
    "    # real_pred = pred[:,0,:]\n",
    "\n",
    "    # 2. use mean of pred as real pred\n",
    "    # pred_middata_np = np.zeros((pred.shape[0], pred.shape[0] + pred.shape[1] - 1, pred.shape[2]))\n",
    "    # pred_middata_np[:,:,:] = np.nan\n",
    "    # for i in range(pred.shape[0]):\n",
    "    #     pred_middata_np[i, i:i+pred.shape[1], :] = pred[i]\n",
    "    # real_pred = np.nanmean(pred_middata_np, axis=0, keepdims=False)\n",
    "\n",
    "    # 3. use mean of pred as real pred, but only use the first 20 pred\n",
    "    real_pred = pred[:, :20, :].mean(axis=1, keepdims=False)\n",
    "\n",
    "    pred_df = pd.DataFrame(real_pred, index=target_label.index[:real_pred.shape[0]], columns=target_label.columns)\n",
    "\n",
    "    # ---- calculate rolling 20 days return ----\n",
    "    rolling_20_label = target_label.rolling(20).mean().shift(-19).iloc(axis=0)[:-19]\n",
    "    average_ret = rolling_20_label.mean(axis=1).to_frame(\"average_ret\")\n",
    "\n",
    "    past_rolling_20_label = target_label.rolling(20, closed=\"left\", min_periods=1).mean()\n",
    "\n",
    "    follow_past_20days_50p = rolling_20_label[(past_rolling_20_label.rank(axis=1, pct=True) > 0.5)].mean(axis=1).to_frame(\"follow_past_20days_50p\")\n",
    "    follow_past_20days_20p = rolling_20_label[(past_rolling_20_label.rank(axis=1, pct=True) > 0.8)].mean(axis=1).to_frame(\"follow_past_20days_20p\")\n",
    "    follow_past_20days_10p = rolling_20_label[(past_rolling_20_label.rank(axis=1, pct=True) > 0.9)].mean(axis=1).to_frame(\"follow_past_20days_10p\")\n",
    "\n",
    "    rolling_20_pred_df = pred_df.rolling(20).mean().shift(-20).iloc(axis=0)[:-20]\n",
    "    top50p_pred_ret = rolling_20_label[(pred_df.rank(axis=1, pct=True) > 0.5)].mean(axis=1).to_frame(\"top50p_pred_ret\")\n",
    "    top20p_pred_ret = rolling_20_label[(pred_df.rank(axis=1, pct=True) > 0.8)].mean(axis=1).to_frame(\"top20p_pred_ret\")\n",
    "    top10p_pred_ret = rolling_20_label[(pred_df.rank(axis=1, pct=True) > 0.9)].mean(axis=1).to_frame(\"top10p_pred_ret\")\n",
    "    show_df = pd.concat([average_ret, top50p_pred_ret, top20p_pred_ret, top10p_pred_ret, follow_past_20days_50p, follow_past_20days_20p, follow_past_20days_10p], axis=1).cumsum(axis=0)\n",
    "    show_df.plot()\n",
    "    plt.title(f\"{dataset_country} {dataset} {model} {seq_length}\")\n",
    "    # plt.show()\n",
    "\n",
    "    # ---- plot pred and label in same photo ----\n",
    "    # pred_with_input = pd.concat([label.iloc[:-test_length,:], pred_df], axis=0)\n",
    "    # pred_with_input.columns = [col + \"_pred\" for col in pred_with_input.columns]\n",
    "    # fig, axes = plt.subplots(5, 5)\n",
    "    # fig.set_figheight(30)\n",
    "    # fig.set_figwidth(30)\n",
    "    # plt.subplots_adjust(wspace=None, hspace=None)\n",
    "    # axes = axes.reshape(5, 5)\n",
    "    # stride = label.shape[-1] // 25\n",
    "    # for i in range(25):\n",
    "    #     pred_and_gt = pd.concat([label.iloc(axis=1)[i*stride:i*stride+1], pred_with_input.iloc(axis=1)[i*stride:i*stride+1]], axis=1).cumsum(axis=0)\n",
    "    #     pred_and_gt.plot(ax=axes[i//5, i%5], legend=True)\n",
    "    #     axes[i//5, i%5].set_title(label.columns[i*stride])\n",
    "    # plt.title(f\"{dataset_country} {dataset} {model} {seq_length}\")\n",
    "    plt.savefig(rf\"/home/xuyang1/PatchTST/PatchTST_self_supervised/plot_result/{dataset_country}_{dataset}_{model}_{seq_length}.png\")\n",
    "    # plt.show()\n",
    "    \n",
    "    # rics = []\n",
    "    # for col in pred_df.columns:\n",
    "    #     rics.append(pred_df[col].corr(target_label[col], method=\"spearman\"))\n",
    "\n",
    "    # print(dataset_country, dataset, seq_length, model, sum(rics) / len(rics), sep=\",\")\n",
    "    plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "patchtst",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3e1fed312b402d124a26fd1f35e64e518612f0eec7447ca06c3f2decdc2741cd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
