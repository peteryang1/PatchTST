{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 处理A股数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "new_data = pickle.load(open(r'/home/xuyang1/PatchTST/our_align_data.lm.V06.pickle/new_data.pkl', 'rb'))\n",
    "\n",
    "# data = pickle.load(open(r'/home/xuyang1/PatchTST/our_align_data.lm.V06.pickle/share.pkl', 'rb'))\n",
    "# ret = pickle.load(open(r'/home/xuyang1/PatchTST/ret.pkl', 'rb'))\n",
    "\n",
    "# ret = ret.reindex(data.index)\n",
    "# new_data = pd.concat([data.iloc(axis=1)[:-5], ret], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for col in tqdm(new_data.columns):\n",
    "#     tmp_df = pd.DataFrame({\"left\": new_data[col], \"right\": new_data.iloc(axis=1)[-5]})\n",
    "#     ic1 = tmp_df.groupby(\"datetime\").apply(lambda df: df[\"left\"].corr(df[\"right\"])).mean()\n",
    "#     tmp_df = pd.DataFrame({\"left\": new_data[col], \"right\": new_data.iloc(axis=1)[-3]})\n",
    "#     ic2 = tmp_df.groupby(\"datetime\").apply(lambda df: df[\"left\"].corr(df[\"right\"])).mean()\n",
    "#     if ic1 < 0 and ic2 < 0:\n",
    "#         new_data[col] *= -1\n",
    "#         print(col, ic1.mean(), ic2.mean())\n",
    "    # assert ic1 > 0 == ic2 > 0\n",
    "\n",
    "#     ic = df.groupby(\"datetime\").apply(lambda df: df[\"pred\"].corr(df[\"label\"]))\n",
    "\n",
    "\n",
    "# for index, col in enumerate(data.columns):\n",
    "#     print(index, col)\n",
    "# for index, col in enumerate(ret.columns):\n",
    "#     print(index, col)\n",
    "\n",
    "# for index in range(5):\n",
    "#     pred = data.iloc[:, -5 + index]\n",
    "#     label = ret.iloc[:, index]\n",
    "#     label = label.reindex(pred.index)\n",
    "#     print(pred.shape, label.shape)\n",
    "#     df = pd.DataFrame({\"pred\": pred, \"label\": label})\n",
    "#     ic = df.groupby(\"datetime\").apply(lambda df: df[\"pred\"].corr(df[\"label\"]))\n",
    "#     ric = df.groupby(\"datetime\").apply(lambda df: df[\"pred\"].corr(df[\"label\"], method=\"spearman\"))\n",
    "#     print(ic.mean(), ric.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "display(new_data.shape)\n",
    "display(new_data.head())\n",
    "# pickle.dump(new_data, open(r'/home/xuyang1/PatchTST/our_align_data.lm.V06.pickle/new_data.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandarallel import pandarallel\n",
    "pandarallel.initialize()\n",
    "\n",
    "tqdm.pandas()\n",
    "def calculate_quintile_ret(df):\n",
    "    label = df.iloc(axis=1)[-5]\n",
    "    quintile_ret = pd.Series(index=df.columns)\n",
    "    for col in df.columns:\n",
    "        tmp_df = pd.DataFrame({\"left\": df[col], \"right\": label}).sort_values(\"left\", ascending=False)\n",
    "        group1_ret = tmp_df.iloc[:int(len(tmp_df) * 0.2), 1].mean()\n",
    "        group5_ret = tmp_df.iloc[int(len(tmp_df) * 0.8):, 1].mean()\n",
    "        quintile_ret[col] = group1_ret - group5_ret\n",
    "\n",
    "    return quintile_ret\n",
    "\n",
    "# quintile_ret_data = new_data.groupby(\"datetime\").parallel_apply(calculate_quintile_ret)\n",
    "# pickle.dump(quintile_ret_data, open(r'/home/xuyang1/PatchTST/our_align_data.lm.V06.pickle/quintile_ret_data.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandarallel import pandarallel\n",
    "pandarallel.initialize()\n",
    "\n",
    "def calculate_top10p_ret(df):\n",
    "    label = df.iloc(axis=1)[-5]\n",
    "    top10p_ret = pd.Series(index=df.columns)\n",
    "    for col in df.columns:\n",
    "        tmp_df = pd.DataFrame({\"left\": df[col], \"right\": label}).sort_values(\"left\", ascending=False)\n",
    "        top10p_ret[col] = tmp_df.iloc[:int(len(tmp_df) * 0.1), 1].mean()\n",
    "    return top10p_ret\n",
    "\n",
    "# top10p_ret_data = new_data.groupby(\"datetime\").parallel_apply(calculate_top10p_ret)\n",
    "# pickle.dump(top10p_ret_data, open(r'/home/xuyang1/PatchTST/our_align_data.lm.V06.pickle/top10p_ret_data.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# top10p_ret_data = pickle.load(open(r'/home/xuyang1/PatchTST/our_align_data.lm.V06.pickle/top10p_ret_data.pkl', 'rb'))\n",
    "# top10p_ret_data.columns = [col[1] for col in top10p_ret_data.columns]\n",
    "# import plotly.express as px\n",
    "# top10p_ret_data_cumsum = top10p_ret_data.cumsum().iloc(axis=1)[::5]\n",
    "# fig = px.line(top10p_ret_data_cumsum, title=\"Top 10% cumulative return\")\n",
    "# fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quintile_ret_data = pickle.load(open(r'/home/xuyang1/PatchTST/our_align_data.lm.V06.pickle/quintile_ret_data.pkl', 'rb'))\n",
    "top10p_ret_data = pickle.load(open(r'/home/xuyang1/PatchTST/our_align_data.lm.V06.pickle/top10p_ret_data.pkl', 'rb'))\n",
    "\n",
    "quintile_ret_data_nolabel = quintile_ret_data.iloc(axis=1)[:-5]\n",
    "top10p_ret_data_nolabel = top10p_ret_data.iloc(axis=1)[:-5]\n",
    "quintile_ret_data_nolabel.columns = [col[1] for col in quintile_ret_data_nolabel.columns]\n",
    "top10p_ret_data_nolabel.columns = [col[1] for col in top10p_ret_data_nolabel.columns]\n",
    "quintile_ret_data_nolabel.index.names = [\"date\"]\n",
    "top10p_ret_data_nolabel.index.names = [\"date\"]\n",
    "\n",
    "quintile_ret_data_nolabel.to_csv(r'/home/xuyang1/PatchTST/PatchTST_supervised/dataset/quintile_stocka.csv')\n",
    "top10p_ret_data_nolabel.to_csv(r'/home/xuyang1/PatchTST/PatchTST_supervised/dataset/top10p_us.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "shape_series = pd.Series()\n",
    "import plotly.express as px\n",
    "for name in os.listdir(r'/home/xuyang1/PatchTST/PatchTST_supervised/dataset/'):\n",
    "    tmp = pd.read_csv(os.path.join(r'/home/xuyang1/PatchTST/PatchTST_supervised/dataset/', name))\n",
    "    shape_series[name] = tmp.shape\n",
    "    if \"stocka\" in name:\n",
    "        tmp = tmp.set_index(\"date\")\n",
    "        tmp = tmp.rolling(20).mean()\n",
    "        fig = px.line(tmp.iloc(axis=1)[::10], title=f\"{name} Top 10% cumulative return\")\n",
    "        fig.show()\n",
    "    \n",
    "# display(shape_series.to_frame(\"dataset_size\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 验证结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "label_stocka_quintile = pd.read_csv(r'/home/xuyang1/PatchTST/PatchTST_supervised/dataset/quintile_stocka.csv', index_col=0)\n",
    "label_us_quintile = pd.read_csv(r'/home/xuyang1/PatchTST/PatchTST_supervised/dataset/quintile_us.csv', index_col=0)\n",
    "# label_stocka_quintile = pd.DataFrame(pickle.load(open(r'/home/xuyang1/PatchTST/PatchTST_supervised/dataset/quintile_stocka_transformed.pkl', 'rb')), index=label_stocka_quintile.index, columns=label_stocka_quintile.columns)\n",
    "# label_us_quintile = pd.DataFrame(pickle.load(open(r'/home/xuyang1/PatchTST/PatchTST_supervised/dataset/quintile_us_transformed.pkl', 'rb')), index=label_us_quintile.index, columns=label_us_quintile.columns)\n",
    "label_stocka_top10p = pd.read_csv(r'/home/xuyang1/PatchTST/PatchTST_supervised/dataset/top10p_stocka.csv', index_col=0)\n",
    "label_us_top10p = pd.read_csv(r'/home/xuyang1/PatchTST/PatchTST_supervised/dataset/top10p_us.csv', index_col=0)\n",
    "# label_stocka_top10p = pd.DataFrame(pickle.load(open(r'/home/xuyang1/PatchTST/PatchTST_supervised/dataset/top10p_stocka_transformed.pkl', 'rb')), index=label_stocka_top10p.index, columns=label_stocka_top10p.columns)\n",
    "# label_us_top10p = pd.DataFrame(pickle.load(open(r'/home/xuyang1/PatchTST/PatchTST_supervised/dataset/top10p_us_transformed.pkl', 'rb')), index=label_us_top10p.index, columns=label_us_top10p.columns)\n",
    "\n",
    "quintile_stocka_scaler = pickle.load(open(r'/home/xuyang1/PatchTST/PatchTST_supervised/dataset/quintile_stocka_scaler.pkl', 'rb'))\n",
    "top10p_stocka_scaler = pickle.load(open(r'/home/xuyang1/PatchTST/PatchTST_supervised/dataset/top10p_stocka_scaler.pkl', 'rb'))\n",
    "quintile_us_scaler = pickle.load(open(r'/home/xuyang1/PatchTST/PatchTST_supervised/dataset/quintile_us_scaler.pkl', 'rb'))\n",
    "top10p_us_scaler = pickle.load(open(r'/home/xuyang1/PatchTST/PatchTST_supervised/dataset/top10p_us_scaler.pkl', 'rb'))\n",
    "losses = []\n",
    "for name in os.listdir(r\"/home/xuyang1/PatchTST/PatchTST_supervised/results\"):\n",
    "    model = name.split(\"_\")[4]\n",
    "    seq_length = name.split(\"_\")[3]\n",
    "    dataset = name.split(\"_\")[0]\n",
    "    dataset_country = name.split(\"_\")[1]\n",
    "    if dataset == \"quintile\" and dataset_country == \"stocka\":\n",
    "        label = label_stocka_quintile\n",
    "    elif dataset == \"quintile\" and dataset_country == \"us\":\n",
    "        label = label_us_quintile\n",
    "    elif dataset == \"top10p\" and dataset_country == \"stocka\":\n",
    "        label = label_stocka_top10p\n",
    "    elif dataset == \"top10p\" and dataset_country == \"us\":\n",
    "        label = label_us_top10p\n",
    "    pred = np.load(rf\"/home/xuyang1/PatchTST/PatchTST_supervised/results/{name}/pred.npy\")\n",
    "\n",
    "    if dataset == \"quintile\" and dataset_country == \"stocka\":\n",
    "        scaler = quintile_stocka_scaler\n",
    "    elif dataset == \"quintile\" and dataset_country == \"us\":\n",
    "        scaler = quintile_us_scaler\n",
    "    elif dataset == \"top10p\" and dataset_country == \"stocka\":\n",
    "        scaler = top10p_stocka_scaler\n",
    "    elif dataset == \"top10p\" and dataset_country == \"us\":\n",
    "        scaler = top10p_us_scaler\n",
    "    pred = scaler.inverse_transform(np.reshape(pred, (-1, pred.shape[-1]))).reshape(pred.shape)\n",
    "\n",
    "    # if \"quinti\" not in dataset or seq_length != \"96\":\n",
    "    #     continue\n",
    "\n",
    "    test_length = int(len(label) * 0.2)\n",
    "\n",
    "    for i in range(pred.shape[-1]):\n",
    "        if label.values[:-test_length,i].mean() < 0:\n",
    "            label.iloc[:,i] = label.iloc[:,i] * -1\n",
    "            pred[:,:,i] = -pred[:,:,i]\n",
    "    target_label = label.iloc[-test_length:,:]\n",
    "    if pred.shape[0] > target_label.shape[0]:\n",
    "        pred = pred[:target_label.shape[0],:,:]\n",
    "    elif pred.shape[0] < target_label.shape[0]:\n",
    "        target_label = target_label[:pred.shape[0],:]\n",
    "\n",
    "\n",
    "    # calculate pred to real label loss:\n",
    "    # loss = 0\n",
    "    # for i in range(pred.shape[0]):\n",
    "    #     pred_batch = pred[i]\n",
    "    #     label_batch = target_label.iloc[i:i+pred.shape[1],:].values\n",
    "    #     loss += np.mean((pred_batch - label_batch)**2)\n",
    "\n",
    "    # calculate real return with prediction:\n",
    "    # 1. use closest pred as real pread\n",
    "    # real_pred = pred[:,0,:]\n",
    "\n",
    "    # 2. use mean of pred as real pred\n",
    "    # pred_middata_np = np.zeros((pred.shape[0], pred.shape[0] + pred.shape[1] - 1, pred.shape[2]))\n",
    "    # pred_middata_np[:,:,:] = np.nan\n",
    "    # for i in range(pred.shape[0]):\n",
    "    #     pred_middata_np[i, i:i+pred.shape[1], :] = pred[i]\n",
    "    # real_pred = np.nanmean(pred_middata_np, axis=0, keepdims=False)\n",
    "\n",
    "    # 3. use mean of pred as real pred, but only use the first 20 pred\n",
    "    real_pred = pred[:, :20, :].mean(axis=1, keepdims=False)\n",
    "\n",
    "    pred_df = pd.DataFrame(real_pred, index=target_label.index[:real_pred.shape[0]], columns=target_label.columns)\n",
    "\n",
    "    # ---- calculate rolling 20 days return ----\n",
    "    rolling_20_label = target_label.rolling(20).mean().shift(-19).iloc(axis=0)[:-19]\n",
    "    average_ret = rolling_20_label.mean(axis=1).to_frame(\"average_ret\")\n",
    "\n",
    "    past_rolling_20_label = target_label.rolling(20, closed=\"left\", min_periods=1).mean()\n",
    "\n",
    "    follow_past_20days_50p = rolling_20_label[(past_rolling_20_label.rank(axis=1, pct=True) > 0.5)].mean(axis=1).to_frame(\"follow_past_20days_50p\")\n",
    "    follow_past_20days_20p = rolling_20_label[(past_rolling_20_label.rank(axis=1, pct=True) > 0.8)].mean(axis=1).to_frame(\"follow_past_20days_20p\")\n",
    "    follow_past_20days_10p = rolling_20_label[(past_rolling_20_label.rank(axis=1, pct=True) > 0.9)].mean(axis=1).to_frame(\"follow_past_20days_10p\")\n",
    "\n",
    "    rolling_20_pred_df = pred_df.rolling(20).mean().shift(-20).iloc(axis=0)[:-20]\n",
    "    top50p_pred_ret = rolling_20_label[(pred_df.rank(axis=1, pct=True) > 0.5)].mean(axis=1).to_frame(\"top50p_pred_ret\")\n",
    "    top20p_pred_ret = rolling_20_label[(pred_df.rank(axis=1, pct=True) > 0.8)].mean(axis=1).to_frame(\"top20p_pred_ret\")\n",
    "    top10p_pred_ret = rolling_20_label[(pred_df.rank(axis=1, pct=True) > 0.9)].mean(axis=1).to_frame(\"top10p_pred_ret\")\n",
    "    show_df = pd.concat([average_ret, top50p_pred_ret, top20p_pred_ret, top10p_pred_ret, follow_past_20days_50p, follow_past_20days_20p, follow_past_20days_10p], axis=1).cumsum(axis=0)\n",
    "    show_df.plot()\n",
    "    plt.title(f\"{dataset_country} {dataset} {model} {seq_length}\")\n",
    "    # plt.show()\n",
    "\n",
    "    # ---- plot pred and label in same photo ----\n",
    "    # pred_with_input = pd.concat([label.iloc[:-test_length,:], pred_df], axis=0)\n",
    "    # pred_with_input.columns = [col + \"_pred\" for col in pred_with_input.columns]\n",
    "    # fig, axes = plt.subplots(5, 5)\n",
    "    # fig.set_figheight(30)\n",
    "    # fig.set_figwidth(30)\n",
    "    # plt.subplots_adjust(wspace=None, hspace=None)\n",
    "    # axes = axes.reshape(5, 5)\n",
    "    # stride = label.shape[-1] // 25\n",
    "    # for i in range(25):\n",
    "    #     pred_and_gt = pd.concat([label.iloc(axis=1)[i*stride:i*stride+1], pred_with_input.iloc(axis=1)[i*stride:i*stride+1]], axis=1).cumsum(axis=0)\n",
    "    #     pred_and_gt.plot(ax=axes[i//5, i%5], legend=True)\n",
    "    #     axes[i//5, i%5].set_title(label.columns[i*stride])\n",
    "    # plt.title(f\"{dataset_country} {dataset} {model} {seq_length}\")\n",
    "    plt.savefig(rf\"/home/xuyang1/PatchTST/PatchTST_supervised/plot_result/{dataset_country}_{dataset}_{model}_{seq_length}.png\")\n",
    "    # plt.show()\n",
    "    \n",
    "    # rics = []\n",
    "    # for col in pred_df.columns:\n",
    "    #     rics.append(pred_df[col].corr(target_label[col], method=\"spearman\"))\n",
    "\n",
    "    # print(dataset_country, dataset, seq_length, model, sum(rics) / len(rics), sep=\",\")\n",
    "    plt.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 处理美股数据\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm \n",
    "from functools import partial\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "msn_data = pd.read_parquet(r\"/home/xuyang1/PatchTST/es_data/msn_data.parq\")\n",
    "price = pd.read_parquet(r\"/home/xuyang1/PatchTST/es_data/orig_price_info.parquet\")\n",
    "\n",
    "msn_data = msn_data.iloc(axis=1)[:-1]\n",
    "msn_data.columns = [i[1] for i in msn_data.columns]\n",
    "def guess_price(df, shift, horizon):\n",
    "    return df.price.shift(-shift-horizon) / df.price.shift(-shift) - 1\n",
    "\n",
    "tqdm.pandas()\n",
    "price[\"price_1d\"] = price.groupby(\"instrument\", group_keys=False).progress_apply(partial(guess_price, shift=1, horizon=1)).sort_index()\n",
    "new_data = msn_data.join(price[\"price_1d\"], how=\"left\").dropna(subset=['price_1d'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandarallel import pandarallel\n",
    "pandarallel.initialize()\n",
    "\n",
    "tqdm.pandas()\n",
    "def calculate_quintile_ret(df):\n",
    "    label = df.iloc(axis=1)[-1]\n",
    "    quintile_ret = pd.Series(index=df.columns)\n",
    "    for col in df.columns:\n",
    "        tmp_df = pd.DataFrame({\"left\": df[col], \"right\": label}).sort_values(\"left\", ascending=False)\n",
    "        group1_ret = tmp_df.iloc[:int(len(tmp_df) * 0.2), 1].mean()\n",
    "        group5_ret = tmp_df.iloc[int(len(tmp_df) * 0.8):, 1].mean()\n",
    "        quintile_ret[col] = group1_ret - group5_ret\n",
    "\n",
    "    return quintile_ret\n",
    "\n",
    "quintile_ret_data = new_data.groupby(\"datetime\").parallel_apply(calculate_quintile_ret)\n",
    "pickle.dump(quintile_ret_data, open(r'/home/xuyang1/PatchTST/es_data/quintile_ret_data.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandarallel import pandarallel\n",
    "pandarallel.initialize()\n",
    "\n",
    "def calculate_top10p_ret(df):\n",
    "    label = df.iloc(axis=1)[-1]\n",
    "    top10p_ret = pd.Series(index=df.columns)\n",
    "    for col in df.columns:\n",
    "        tmp_df = pd.DataFrame({\"left\": df[col], \"right\": label}).sort_values(\"left\", ascending=False)\n",
    "        top10p_ret[col] = tmp_df.iloc[:int(len(tmp_df) * 0.1), 1].mean()\n",
    "    return top10p_ret\n",
    "\n",
    "top10p_ret_data = new_data.groupby(\"datetime\").parallel_apply(calculate_top10p_ret)\n",
    "pickle.dump(top10p_ret_data, open(r'/home/xuyang1/PatchTST/es_data/top10p_ret_data.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quintile_ret_data = pickle.load(open(r'/home/xuyang1/PatchTST/es_data/quintile_ret_data.pkl', 'rb'))\n",
    "top10p_ret_data = pickle.load(open(r'/home/xuyang1/PatchTST/es_data/top10p_ret_data.pkl', 'rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in tqdm(quintile_ret_data.columns):\n",
    "    count = 0\n",
    "    while True:\n",
    "        upper = quintile_ret_data[col].mean() + quintile_ret_data[col].std() * 5\n",
    "        lower = quintile_ret_data[col].mean() - quintile_ret_data[col].std() * 5\n",
    "        if (upper > quintile_ret_data[col].max()) and (lower < quintile_ret_data[col].min()):\n",
    "            break\n",
    "        if count > 3:\n",
    "            break\n",
    "        count += 1\n",
    "        quintile_ret_data[col] = quintile_ret_data[col].clip(lower, upper,)\n",
    "\n",
    "for col in tqdm(top10p_ret_data.columns):\n",
    "    count = 0\n",
    "    while True:\n",
    "        upper = top10p_ret_data[col].mean() + top10p_ret_data[col].std() * 5\n",
    "        lower = top10p_ret_data[col].mean() - top10p_ret_data[col].std() * 5\n",
    "        if (upper > top10p_ret_data[col].max()) and (lower < top10p_ret_data[col].min()):\n",
    "            break\n",
    "        if count > 3:\n",
    "            break\n",
    "        count += 1\n",
    "        top10p_ret_data[col] = top10p_ret_data[col].clip(lower, upper,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quintile_ret_data = quintile_ret_data.iloc(axis=1)[:-1]\n",
    "top10p_ret_data = top10p_ret_data.iloc(axis=1)[:-1]\n",
    "\n",
    "quintile_ret_data.index.names = [\"date\"]\n",
    "top10p_ret_data.index.names = [\"date\"]\n",
    "\n",
    "quintile_ret_data.to_csv(r'/home/xuyang1/PatchTST/PatchTST_supervised/dataset/quintile_us.csv')\n",
    "top10p_ret_data.to_csv(r'/home/xuyang1/PatchTST/PatchTST_supervised/dataset/top10p_us.csv')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 记录所有loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "lines = open(r\"/home/xuyang1/PatchTST/final_result.txt\").readlines()\n",
    "\n",
    "result_df = pd.DataFrame(columns=[\"dataset\", \"country\", \"seq_length\", \"model\", \"mse\", \"mae\", \"rse\"])\n",
    "for line in lines:\n",
    "    line = line.strip()\n",
    "    if len(line) == 0:\n",
    "        continue\n",
    "    dataset = line.split(\"_\")[0]\n",
    "    dataset_country = line.split(\"_\")[1]\n",
    "    seq_len = int(line.split(\"_\")[3])\n",
    "    model = line.split(\"_\")[4]\n",
    "    mse = float(line.split(\"mse:\")[1].split(\",\")[0])\n",
    "    mae = float(line.split(\"mae:\")[1].split(\",\")[0])\n",
    "    rse = float(line.split(\"rse:\")[1].split(\",\")[0])\n",
    "    result_df = pd.concat([result_df, pd.DataFrame([[dataset, dataset_country, seq_len, model, mse, mae, rse]], columns=result_df.columns)], axis=0)\n",
    "\n",
    "result_df = result_df.set_index([\"dataset\", \"country\", \"seq_length\", \"model\"]).sort_index()\n",
    "result_df.to_excel(r\"/home/xuyang1/PatchTST/final_result.xlsx\")\n",
    "# display(result_df)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 拼接所有图片"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "file_names = os.listdir(r\"/home/xuyang1/PatchTST/PatchTST_supervised/plot_result\")\n",
    "\n",
    "result_df = pd.DataFrame(columns=[\"dataset\", \"country\", \"seq_length\", \"model\", \"name\"])\n",
    "for original_name in file_names:\n",
    "    name = original_name.split(\".\")[0]\n",
    "    dataset_country = name.split(\"_\")[0]\n",
    "    dataset = name.split(\"_\")[1]\n",
    "    model = name.split(\"_\")[2]\n",
    "    seq_len = int(name.split(\"_\")[3])\n",
    "    \n",
    "    result_df = pd.concat([result_df, pd.DataFrame([[dataset, dataset_country, seq_len, model, f\"/home/xuyang1/PatchTST/PatchTST_supervised/plot_result/{original_name}\"]], columns=result_df.columns)], axis=0)\n",
    "\n",
    "\n",
    "\n",
    "result_df = result_df.set_index([\"dataset\", \"country\", \"model\", \"seq_length\", ]).sort_index()\n",
    "# result_df = result_df.set_index([\"model\"])\n",
    "for dataset in result_df.index.get_level_values(\"dataset\").drop_duplicates():\n",
    "    part_df1 = result_df.loc[dataset]\n",
    "    for country in part_df1.index.get_level_values(\"country\").drop_duplicates():\n",
    "        part_df2 = part_df1.loc[country]\n",
    "        images = []\n",
    "        for model in part_df2.index.get_level_values(\"model\").drop_duplicates():\n",
    "            part_df3 = part_df2.loc[model]\n",
    "            for seq_length in part_df3.index.get_level_values(\"seq_length\").drop_duplicates():\n",
    "                part_df4 = part_df3.loc(axis=0)[seq_length]\n",
    "                img = cv2.imread(part_df4[\"name\"])\n",
    "                cv2.putText(img, f\"{dataset}_{country}_{seq_length}_{model}\", (0, 100), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "                images.append(img)\n",
    "        height, width, _ = images[0].shape  \n",
    "        row_num = len(images) // 4\n",
    "        col_num = 4\n",
    "    \n",
    "        # 创建一个空白图片，用于存放拼接后的图片  \n",
    "        result_image = np.zeros((height * row_num, width * col_num, 3), np.uint8)  \n",
    "    \n",
    "        # 遍历每个图片，按照九宫格的顺序将其粘贴到结果图片上  \n",
    "        for i, image in enumerate(images):  \n",
    "            x = i % col_num * width  \n",
    "            y = i // col_num * height  \n",
    "            result_image[y:y+height, x:x+width] = image\n",
    "        result_image = cv2.resize(result_image, (result_image.shape[1], result_image.shape[0]))\n",
    "        cv2.imwrite(f'/home/xuyang1/PatchTST/PatchTST_supervised/plot_result_combined/{dataset}_{country}.png', result_image,)\n",
    "\n",
    "\n",
    "\n",
    "display(result_df)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 对比 self supervised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "label_stocka_quintile = pd.read_csv(r'/home/xuyang1/PatchTST/PatchTST_supervised/dataset/quintile_stocka.csv', index_col=0)\n",
    "label_us_quintile = pd.read_csv(r'/home/xuyang1/PatchTST/PatchTST_supervised/dataset/quintile_us.csv', index_col=0)\n",
    "# label_stocka_quintile = pd.DataFrame(pickle.load(open(r'/home/xuyang1/PatchTST/PatchTST_supervised/dataset/quintile_stocka_transformed.pkl', 'rb')), index=label_stocka_quintile.index, columns=label_stocka_quintile.columns)\n",
    "# label_us_quintile = pd.DataFrame(pickle.load(open(r'/home/xuyang1/PatchTST/PatchTST_supervised/dataset/quintile_us_transformed.pkl', 'rb')), index=label_us_quintile.index, columns=label_us_quintile.columns)\n",
    "label_stocka_top10p = pd.read_csv(r'/home/xuyang1/PatchTST/PatchTST_supervised/dataset/top10p_stocka.csv', index_col=0)\n",
    "label_us_top10p = pd.read_csv(r'/home/xuyang1/PatchTST/PatchTST_supervised/dataset/top10p_us.csv', index_col=0)\n",
    "# label_stocka_top10p = pd.DataFrame(pickle.load(open(r'/home/xuyang1/PatchTST/PatchTST_supervised/dataset/top10p_stocka_transformed.pkl', 'rb')), index=label_stocka_top10p.index, columns=label_stocka_top10p.columns)\n",
    "# label_us_top10p = pd.DataFrame(pickle.load(open(r'/home/xuyang1/PatchTST/PatchTST_supervised/dataset/top10p_us_transformed.pkl', 'rb')), index=label_us_top10p.index, columns=label_us_top10p.columns)\n",
    "\n",
    "quintile_stocka_scaler = pickle.load(open(r'/home/xuyang1/PatchTST/PatchTST_supervised/dataset/quintile_stocka_scaler.pkl', 'rb'))\n",
    "top10p_stocka_scaler = pickle.load(open(r'/home/xuyang1/PatchTST/PatchTST_supervised/dataset/top10p_stocka_scaler.pkl', 'rb'))\n",
    "quintile_us_scaler = pickle.load(open(r'/home/xuyang1/PatchTST/PatchTST_supervised/dataset/quintile_us_scaler.pkl', 'rb'))\n",
    "top10p_us_scaler = pickle.load(open(r'/home/xuyang1/PatchTST/PatchTST_supervised/dataset/top10p_us_scaler.pkl', 'rb'))\n",
    "losses = []\n",
    "for name in os.listdir(r\"/home/xuyang1/PatchTST/PatchTST_self_supervised/results\"):\n",
    "    if \"finetune\" in name or \"linear-probe\" in name:\n",
    "        model = \"PatchTST\" + name.split(\"_\")[3]\n",
    "        seq_length = name.split(\"tw\")[-1].split(\"_\")[0]\n",
    "        dataset = name.split(\"_\")[0]\n",
    "        dataset_country = name.split(\"_\")[1]\n",
    "    else:\n",
    "        model = name.split(\"_\")[4]\n",
    "        seq_length = name.split(\"_\")[3]\n",
    "        dataset = name.split(\"_\")[0]\n",
    "        dataset_country = name.split(\"_\")[1]\n",
    "    if dataset == \"quintile\" and dataset_country == \"stocka\":\n",
    "        label = label_stocka_quintile\n",
    "    elif dataset == \"quintile\" and dataset_country == \"us\":\n",
    "        label = label_us_quintile\n",
    "    elif dataset == \"top10p\" and dataset_country == \"stocka\":\n",
    "        label = label_stocka_top10p\n",
    "    elif dataset == \"top10p\" and dataset_country == \"us\":\n",
    "        label = label_us_top10p\n",
    "    pred = np.load(rf\"/home/xuyang1/PatchTST/PatchTST_self_supervised/results/{name}/pred.npy\")\n",
    "    # if \"stocka\" == dataset_country:\n",
    "    #     pred = pred[:593, :, :]\n",
    "    # else:\n",
    "    #     pred = pred[:986, :, :]\n",
    "\n",
    "    if dataset == \"quintile\" and dataset_country == \"stocka\":\n",
    "        scaler = quintile_stocka_scaler\n",
    "    elif dataset == \"quintile\" and dataset_country == \"us\":\n",
    "        scaler = quintile_us_scaler\n",
    "    elif dataset == \"top10p\" and dataset_country == \"stocka\":\n",
    "        scaler = top10p_stocka_scaler\n",
    "    elif dataset == \"top10p\" and dataset_country == \"us\":\n",
    "        scaler = top10p_us_scaler\n",
    "    pred = scaler.inverse_transform(np.reshape(pred, (-1, pred.shape[-1]))).reshape(pred.shape)\n",
    "\n",
    "    # if \"quinti\" not in dataset or seq_length != \"96\":\n",
    "    #     continue\n",
    "\n",
    "    test_length = int(len(label) * 0.2)\n",
    "\n",
    "    for i in range(pred.shape[-1]):\n",
    "        if label.values[:-test_length,i].mean() < 0:\n",
    "            label.iloc[:,i] = label.iloc[:,i] * -1\n",
    "            pred[:,:,i] = -pred[:,:,i]\n",
    "    target_label = label.iloc[-test_length:,:]\n",
    "    if pred.shape[0] > target_label.shape[0]:\n",
    "        pred = pred[:target_label.shape[0],:,:]\n",
    "    elif pred.shape[0] < target_label.shape[0]:\n",
    "        target_label = target_label.iloc[:pred.shape[0],:]\n",
    "\n",
    "\n",
    "    # calculate pred to real label loss:\n",
    "    loss = 0\n",
    "    for i in range(pred.shape[0]):\n",
    "        pred_batch = pred[i]\n",
    "        label_batch = target_label.iloc[i:i+pred.shape[1],:].values\n",
    "        loss += np.mean((pred_batch - label_batch)**2)\n",
    "    mse = np.mean((pred[:,0,:] - target_label.values)**2)\n",
    "    mae = np.mean(np.abs(pred[:,0,:] - target_label.values))\n",
    "    print(f\"{model},{dataset},{dataset_country},{seq_length},{mse},{mae}\")\n",
    "    continue\n",
    "\n",
    "    # calculate real return with prediction:\n",
    "    # 1. use closest pred as real pread\n",
    "    # real_pred = pred[:,0,:]\n",
    "\n",
    "    # 2. use mean of pred as real pred\n",
    "    # pred_middata_np = np.zeros((pred.shape[0], pred.shape[0] + pred.shape[1] - 1, pred.shape[2]))\n",
    "    # pred_middata_np[:,:,:] = np.nan\n",
    "    # for i in range(pred.shape[0]):\n",
    "    #     pred_middata_np[i, i:i+pred.shape[1], :] = pred[i]\n",
    "    # real_pred = np.nanmean(pred_middata_np, axis=0, keepdims=False)\n",
    "\n",
    "    # 3. use mean of pred as real pred, but only use the first 20 pred\n",
    "    # real_pred = pred[:, :20, :].mean(axis=1, keepdims=False)\n",
    "\n",
    "    # pred_df = pd.DataFrame(real_pred, index=target_label.index[:real_pred.shape[0]], columns=target_label.columns)\n",
    "\n",
    "    # # ---- calculate rolling 20 days return ----\n",
    "    # rolling_20_label = target_label.rolling(20).mean().shift(-19).iloc(axis=0)[:-19]\n",
    "    # average_ret = rolling_20_label.mean(axis=1).to_frame(\"average_ret\")\n",
    "\n",
    "    # past_rolling_20_label = target_label.rolling(20, closed=\"left\", min_periods=1).mean()\n",
    "\n",
    "    # follow_past_20days_50p = rolling_20_label[(past_rolling_20_label.rank(axis=1, pct=True) > 0.5)].mean(axis=1).to_frame(\"follow_past_20days_50p\")\n",
    "    # follow_past_20days_20p = rolling_20_label[(past_rolling_20_label.rank(axis=1, pct=True) > 0.8)].mean(axis=1).to_frame(\"follow_past_20days_20p\")\n",
    "    # follow_past_20days_10p = rolling_20_label[(past_rolling_20_label.rank(axis=1, pct=True) > 0.9)].mean(axis=1).to_frame(\"follow_past_20days_10p\")\n",
    "\n",
    "    # rolling_20_pred_df = pred_df.rolling(20).mean().shift(-20).iloc(axis=0)[:-20]\n",
    "    # top50p_pred_ret = rolling_20_label[(pred_df.rank(axis=1, pct=True) > 0.5)].mean(axis=1).to_frame(\"top50p_pred_ret\")\n",
    "    # top20p_pred_ret = rolling_20_label[(pred_df.rank(axis=1, pct=True) > 0.8)].mean(axis=1).to_frame(\"top20p_pred_ret\")\n",
    "    # top10p_pred_ret = rolling_20_label[(pred_df.rank(axis=1, pct=True) > 0.9)].mean(axis=1).to_frame(\"top10p_pred_ret\")\n",
    "    # show_df = pd.concat([average_ret, top50p_pred_ret, top20p_pred_ret, top10p_pred_ret, follow_past_20days_50p, follow_past_20days_20p, follow_past_20days_10p], axis=1).cumsum(axis=0)\n",
    "    # show_df.plot()\n",
    "    # plt.title(f\"{dataset_country} {dataset} {model} {seq_length}\")\n",
    "    # plt.show()\n",
    "\n",
    "    # ---- plot pred and label in same photo ----\n",
    "    # pred_with_input = pd.concat([label.iloc[:-test_length,:], pred_df], axis=0)\n",
    "    # pred_with_input.columns = [col + \"_pred\" for col in pred_with_input.columns]\n",
    "    # fig, axes = plt.subplots(5, 5)\n",
    "    # fig.set_figheight(30)\n",
    "    # fig.set_figwidth(30)\n",
    "    # plt.subplots_adjust(wspace=None, hspace=None)\n",
    "    # axes = axes.reshape(5, 5)\n",
    "    # stride = label.shape[-1] // 25\n",
    "    # for i in range(25):\n",
    "    #     pred_and_gt = pd.concat([label.iloc(axis=1)[i*stride:i*stride+1], pred_with_input.iloc(axis=1)[i*stride:i*stride+1]], axis=1).cumsum(axis=0)\n",
    "    #     pred_and_gt.plot(ax=axes[i//5, i%5], legend=True)\n",
    "    #     axes[i//5, i%5].set_title(label.columns[i*stride])\n",
    "    # plt.title(f\"{dataset_country} {dataset} {model} {seq_length}\")\n",
    "    # plt.savefig(rf\"/home/xuyang1/PatchTST/PatchTST_self_supervised/plot_result/{dataset_country}_{dataset}_{model}_{seq_length}.png\")\n",
    "    # plt.show()\n",
    "    \n",
    "    # rics = []\n",
    "    # for col in pred_df.columns:\n",
    "    #     rics.append(pred_df[col].corr(target_label[col], method=\"spearman\"))\n",
    "\n",
    "    # print(dataset_country, dataset, seq_length, model, sum(rics) / len(rics), sep=\",\")\n",
    "    plt.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 对比 fintune and supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  \n",
    "  \n",
    "def r_squared(y_true, y_pred):  \n",
    "    \"\"\"  \n",
    "    Calculate the R-squared (coefficient of determination) for given true and predicted values.  \n",
    "  \n",
    "    :param y_true: list or numpy array of true values  \n",
    "    :param y_pred: list or numpy array of predicted values  \n",
    "    :return: R-squared value  \n",
    "    \"\"\"  \n",
    "    y_true = np.array(y_true)  \n",
    "    y_pred = np.array(y_pred)  \n",
    "  \n",
    "    # Calculate the mean of true values  \n",
    "    y_mean = np.mean(y_true)  \n",
    "  \n",
    "    # Calculate the total sum of squares (TSS)  \n",
    "    tss = np.sum((y_true - y_mean) ** 2)  \n",
    "  \n",
    "    # Calculate the residual sum of squares (RSS)  \n",
    "    rss = np.sum((y_true - y_pred) ** 2)  \n",
    "  \n",
    "    # Calculate R-squared  \n",
    "    r2 = 1 - (rss / tss)  \n",
    "  \n",
    "    return r2  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 对比 ff data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "label_us_quintile = pd.read_csv(r'/home/xuyang1/PatchTST/PatchTST_supervised/dataset/ff_data.csv', index_col=0)\n",
    "# label_us_quintile = pd.DataFrame(pickle.load(open(r'/home/xuyang1/PatchTST/PatchTST_supervised/dataset/ff_data_transformed.pkl', 'rb')), index=label_us_quintile.index, columns=label_us_quintile.columns)\n",
    "\n",
    "pred_supervised = np.load(r\"/home/xuyang1/PatchTST/PatchTST_supervised/results/ff_data_512_96_PatchTST_custom_ftM_sl512_ll48_pl96_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_Exp_0/pred.npy\")\n",
    "\n",
    "pred_linear_prob = np.load(r\"/home/xuyang1/PatchTST/PatchTST_self_supervised/saved_models/ff_data/masked_patchtst/based_model/ff_data_patchtst_linear-probe_cw512_tw96_patch16_stride8_epochs-finetune100_model1/pred.npy\")\n",
    "\n",
    "pred_finetune = np.load(r\"/home/xuyang1/PatchTST/PatchTST_self_supervised/saved_models/ff_data/masked_patchtst/based_model/ff_data_patchtst_finetuned_cw512_tw96_patch16_stride8_epochs-finetune100_model1/pred.npy\")\n",
    "\n",
    "label = label_us_quintile\n",
    "test_length = int(len(label) * 0.2)\n",
    "\n",
    "\n",
    "target_label = label.iloc[-test_length:,:]\n",
    "pred_supervised = pred_supervised[:4997,:,:]\n",
    "pred_linear_prob = pred_linear_prob[:4997,:,:]\n",
    "pred_finetune = pred_finetune[:4997,:,:]\n",
    "ff_scaler = pickle.load(open(r'/home/xuyang1/PatchTST/PatchTST_supervised/dataset/ff_data_scaler.pkl', 'rb'))\n",
    "pred_supervised = ff_scaler.inverse_transform(np.reshape(pred_supervised, (-1, pred_supervised.shape[-1]))).reshape(pred_supervised.shape)\n",
    "pred_linear_prob = ff_scaler.inverse_transform(np.reshape(pred_linear_prob, (-1, pred_linear_prob.shape[-1]))).reshape(pred_linear_prob.shape)\n",
    "pred_finetune = ff_scaler.inverse_transform(np.reshape(pred_finetune, (-1, pred_finetune.shape[-1]))).reshape(pred_finetune.shape)\n",
    "\n",
    "# pred_supervised = np.reshape(pred_supervised[:4997,:], (-1, 3))\n",
    "# pred_linear_prob = np.reshape(pred_linear_prob[:4997,:], (-1, 3))\n",
    "# pred_finetune = np.reshape(pred_finetune[:4997,:], (-1, 3))\n",
    "\n",
    "# display(test_length, target_label.shape, pred_supervised.shape, pred_linear_prob.shape)\n",
    "# target_label_list = [target_label[i:i+96] for i in range(4997)]\n",
    "# target_label_aligned = np.concatenate(target_label_list, axis=0)\n",
    "\n",
    "# loss_supervised = np.mean((target_label_aligned - pred_supervised)**2)\n",
    "# loss_linear_prob = np.mean((target_label_aligned - pred_linear_prob)**2)\n",
    "# loss_finetune = np.mean((target_label_aligned - pred_finetune)**2)\n",
    "\n",
    "# r2_supervised = r_squared(target_label_aligned, pred_supervised)\n",
    "# r2_linear_prob = r_squared(target_label_aligned, pred_linear_prob)\n",
    "# r2_finetune = r_squared(target_label_aligned, pred_finetune)\n",
    "\n",
    "# print(\"loss_supervised:\", loss_supervised, \"loss_linear_prob:\", loss_linear_prob, \"loss_finetune:\", loss_finetune)\n",
    "# print(\"r2_supervised:\", r2_supervised, \"r2_linear_prob:\", r2_linear_prob, \"r2_finetune:\", r2_finetune)\n",
    "show_df = []\n",
    "for pred in [pred_supervised, pred_linear_prob, pred_finetune]:\n",
    "    \n",
    "    if pred is pred_supervised:\n",
    "        title = \"supervised\"\n",
    "    elif pred is pred_linear_prob:\n",
    "        title = \"linear_prob\"\n",
    "    elif pred is pred_finetune:\n",
    "        title = \"finetune\"\n",
    "    real_pred = pred[:, :20, :].mean(axis=1, keepdims=False)\n",
    "                \n",
    "    pred_df = pd.DataFrame(real_pred, index=target_label.index[:real_pred.shape[0]], columns=target_label.columns)\n",
    "\n",
    "    # rolling_20_label = target_label.rolling(20).mean().shift(-19).iloc(axis=0)[:-19] / 30\n",
    "    average_ret = target_label.mean(axis=1).to_frame(\"average_ret\")\n",
    "\n",
    "    past_rolling_20_label = target_label.rolling(20, closed=\"left\", min_periods=1).mean()\n",
    "    # past_rolling_20_label = target_label.shift(30)\n",
    "\n",
    "    display(past_rolling_20_label.shape, target_label.shape)\n",
    "\n",
    "    \n",
    "    factor1 = target_label.iloc[:,0].to_frame(f\"factor1\")\n",
    "    factor2 = target_label.iloc[:,1].to_frame(f\"factor2\")\n",
    "    factor3 = target_label.iloc[:,2].to_frame(f\"factor3\")\n",
    "\n",
    "    display(past_rolling_20_label.head(), target_label.head(), past_rolling_20_label.rank(axis=1).head())\n",
    "\n",
    "    follow_past_20days_50p = target_label[(past_rolling_20_label.rank(axis=1) >= 2)].mean(axis=1).to_frame(f\"{title}_follow_past_20days_top2\")\n",
    "    follow_past_20days_20p = target_label[(past_rolling_20_label.rank(axis=1) >= 3)].mean(axis=1).to_frame(f\"{title}_follow_past_20days_top1\")\n",
    "\n",
    "    # top50p_pred_ret = target_label[(pred_df.rank(axis=1) >= 2)].mean(axis=1).to_frame(f\"{title}_top1_pred_ret\")\n",
    "    # top20p_pred_ret = target_label[(pred_df.rank(axis=1) >= 3)].mean(axis=1).to_frame(f\"{title}_top2_pred_ret\")\n",
    "    show_df = pd.concat([average_ret, factor1, factor2, factor3, follow_past_20days_50p, follow_past_20days_20p], axis=1)\n",
    "    # show_df = pd.concat([average_ret, top50p_pred_ret, top20p_pred_ret, follow_past_20days_50p, follow_past_20days_20p, factor1, factor2, factor3], axis=1)\n",
    "    show_df = show_df.iloc[30:,:]\n",
    "    show_df = show_df.cumsum(axis=0)\n",
    "    # show_df.extend([average_ret, top20p_pred_ret])\n",
    "\n",
    "# show_df = pd.concat(show_df, axis=1).cumsum(axis=0)\n",
    "# show_df = show_df.iloc(axis=1)[[0,1,3,5]]\n",
    "    show_df.plot()\n",
    "    # if pred is pred_supervised:\n",
    "    #     plt.title(\"supervised\")\n",
    "    # elif pred is pred_linear_prob:\n",
    "    #     plt.title(\"linear_prob\")\n",
    "    # elif pred is pred_finetune:\n",
    "    #     plt.title(\"finetune\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "ff_data = pd.read_csv(r\"/home/xuyang1/PatchTST/PatchTST_supervised/dataset/F-F_Research_Data_Factors_daily.CSV\")\n",
    "\n",
    "ff_data.columns = [\"Date\", \"Mkt-RF\", \"SMB\", \"HML\", \"RF\"]\n",
    "ff_data[\"Date\"] = pd.to_datetime(ff_data[\"Date\"], format=\"%Y%m%d\")\n",
    "ff_data.drop(\"RF\", axis=1, inplace=True) # Drop the date \n",
    "ff_data.set_index(\"Date\", inplace=True)\n",
    "\n",
    "display(ff_data.head())\n",
    "\n",
    "ff_data.to_csv(r'/home/xuyang1/PatchTST/PatchTST_self_supervised/data/datasets/ff_data.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 处理一分钟数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 8 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "from pandarallel import pandarallel\n",
    "pandarallel.initialize(nb_workers=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "csi500_instrument = pd.read_csv(r\"/home/xuyang1/PatchTST/csi500.txt\", header=None).set_index(0)\n",
    "csi500_instrument.columns = [\"start_time\", \"end_time\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "data = pickle.load(open(r\"/home/xuyang1/PatchTST/1min_data/preprocessed_dataset_train.pkl\", \"rb\"))\n",
    "def pick_csi500(data, csi500_df):\n",
    "    instrument_id = data.index.get_level_values(\"instrument\").to_list()[0]\n",
    "    if instrument_id not in csi500_df.index:\n",
    "        return pd.DataFrame(columns=data.columns)\n",
    "    else:\n",
    "        csi500_time_period = csi500_df.loc[[instrument_id]]\n",
    "        picked_data = []\n",
    "        for _, row in csi500_time_period.iterrows():\n",
    "            pickled_df = data.loc[instrument_id].loc[row[\"start_time\"]:row[\"end_time\"]]\n",
    "            if len(pickled_df) != 0:\n",
    "                picked_data.append(pickled_df)\n",
    "        if len(picked_data) == 0:\n",
    "            return pd.DataFrame(columns=data.columns)\n",
    "        if len(picked_data) == 1:\n",
    "            return picked_data[0]\n",
    "        else:\n",
    "            picked_data = pd.concat(picked_data, axis=0)\n",
    "            return picked_data\n",
    "\n",
    "# import random\n",
    "# start = random.randint(0, 10000000)\n",
    "\n",
    "data_new = data.groupby(\"instrument\").parallel_apply(pick_csi500, csi500_df=csi500_instrument)\n",
    "display(data_new.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "patchtst",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3e1fed312b402d124a26fd1f35e64e518612f0eec7447ca06c3f2decdc2741cd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
