{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 处理A股数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "new_data = pickle.load(open(r'/home/xuyang1/PatchTST/our_align_data.lm.V06.pickle/new_data.pkl', 'rb'))\n",
    "\n",
    "# data = pickle.load(open(r'/home/xuyang1/PatchTST/our_align_data.lm.V06.pickle/share.pkl', 'rb'))\n",
    "# ret = pickle.load(open(r'/home/xuyang1/PatchTST/ret.pkl', 'rb'))\n",
    "\n",
    "# ret = ret.reindex(data.index)\n",
    "# new_data = pd.concat([data.iloc(axis=1)[:-5], ret], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for col in tqdm(new_data.columns):\n",
    "#     tmp_df = pd.DataFrame({\"left\": new_data[col], \"right\": new_data.iloc(axis=1)[-5]})\n",
    "#     ic1 = tmp_df.groupby(\"datetime\").apply(lambda df: df[\"left\"].corr(df[\"right\"])).mean()\n",
    "#     tmp_df = pd.DataFrame({\"left\": new_data[col], \"right\": new_data.iloc(axis=1)[-3]})\n",
    "#     ic2 = tmp_df.groupby(\"datetime\").apply(lambda df: df[\"left\"].corr(df[\"right\"])).mean()\n",
    "#     if ic1 < 0 and ic2 < 0:\n",
    "#         new_data[col] *= -1\n",
    "#         print(col, ic1.mean(), ic2.mean())\n",
    "    # assert ic1 > 0 == ic2 > 0\n",
    "\n",
    "#     ic = df.groupby(\"datetime\").apply(lambda df: df[\"pred\"].corr(df[\"label\"]))\n",
    "\n",
    "\n",
    "# for index, col in enumerate(data.columns):\n",
    "#     print(index, col)\n",
    "# for index, col in enumerate(ret.columns):\n",
    "#     print(index, col)\n",
    "\n",
    "# for index in range(5):\n",
    "#     pred = data.iloc[:, -5 + index]\n",
    "#     label = ret.iloc[:, index]\n",
    "#     label = label.reindex(pred.index)\n",
    "#     print(pred.shape, label.shape)\n",
    "#     df = pd.DataFrame({\"pred\": pred, \"label\": label})\n",
    "#     ic = df.groupby(\"datetime\").apply(lambda df: df[\"pred\"].corr(df[\"label\"]))\n",
    "#     ric = df.groupby(\"datetime\").apply(lambda df: df[\"pred\"].corr(df[\"label\"], method=\"spearman\"))\n",
    "#     print(ic.mean(), ric.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "display(new_data.shape)\n",
    "display(new_data.head())\n",
    "# pickle.dump(new_data, open(r'/home/xuyang1/PatchTST/our_align_data.lm.V06.pickle/new_data.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandarallel import pandarallel\n",
    "pandarallel.initialize()\n",
    "\n",
    "tqdm.pandas()\n",
    "def calculate_quintile_ret(df):\n",
    "    label = df.iloc(axis=1)[-5]\n",
    "    quintile_ret = pd.Series(index=df.columns)\n",
    "    for col in df.columns:\n",
    "        tmp_df = pd.DataFrame({\"left\": df[col], \"right\": label}).sort_values(\"left\", ascending=False)\n",
    "        group1_ret = tmp_df.iloc[:int(len(tmp_df) * 0.2), 1].mean()\n",
    "        group5_ret = tmp_df.iloc[int(len(tmp_df) * 0.8):, 1].mean()\n",
    "        quintile_ret[col] = group1_ret - group5_ret\n",
    "\n",
    "    return quintile_ret\n",
    "\n",
    "# quintile_ret_data = new_data.groupby(\"datetime\").parallel_apply(calculate_quintile_ret)\n",
    "# pickle.dump(quintile_ret_data, open(r'/home/xuyang1/PatchTST/our_align_data.lm.V06.pickle/quintile_ret_data.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandarallel import pandarallel\n",
    "pandarallel.initialize()\n",
    "\n",
    "def calculate_top10p_ret(df):\n",
    "    label = df.iloc(axis=1)[-5]\n",
    "    top10p_ret = pd.Series(index=df.columns)\n",
    "    for col in df.columns:\n",
    "        tmp_df = pd.DataFrame({\"left\": df[col], \"right\": label}).sort_values(\"left\", ascending=False)\n",
    "        top10p_ret[col] = tmp_df.iloc[:int(len(tmp_df) * 0.1), 1].mean()\n",
    "    return top10p_ret\n",
    "\n",
    "# top10p_ret_data = new_data.groupby(\"datetime\").parallel_apply(calculate_top10p_ret)\n",
    "# pickle.dump(top10p_ret_data, open(r'/home/xuyang1/PatchTST/our_align_data.lm.V06.pickle/top10p_ret_data.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# top10p_ret_data = pickle.load(open(r'/home/xuyang1/PatchTST/our_align_data.lm.V06.pickle/top10p_ret_data.pkl', 'rb'))\n",
    "# top10p_ret_data.columns = [col[1] for col in top10p_ret_data.columns]\n",
    "# import plotly.express as px\n",
    "# top10p_ret_data_cumsum = top10p_ret_data.cumsum().iloc(axis=1)[::5]\n",
    "# fig = px.line(top10p_ret_data_cumsum, title=\"Top 10% cumulative return\")\n",
    "# fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quintile_ret_data = pickle.load(open(r'/home/xuyang1/PatchTST/our_align_data.lm.V06.pickle/quintile_ret_data.pkl', 'rb'))\n",
    "top10p_ret_data = pickle.load(open(r'/home/xuyang1/PatchTST/our_align_data.lm.V06.pickle/top10p_ret_data.pkl', 'rb'))\n",
    "\n",
    "quintile_ret_data_nolabel = quintile_ret_data.iloc(axis=1)[:-5]\n",
    "top10p_ret_data_nolabel = top10p_ret_data.iloc(axis=1)[:-5]\n",
    "quintile_ret_data_nolabel.columns = [col[1] for col in quintile_ret_data_nolabel.columns]\n",
    "top10p_ret_data_nolabel.columns = [col[1] for col in top10p_ret_data_nolabel.columns]\n",
    "quintile_ret_data_nolabel.index.names = [\"date\"]\n",
    "top10p_ret_data_nolabel.index.names = [\"date\"]\n",
    "\n",
    "quintile_ret_data_nolabel.to_csv(r'/home/xuyang1/PatchTST/PatchTST_supervised/dataset/quintile_stocka.csv')\n",
    "top10p_ret_data_nolabel.to_csv(r'/home/xuyang1/PatchTST/PatchTST_supervised/dataset/top10p_us.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "shape_series = pd.Series()\n",
    "import plotly.express as px\n",
    "for name in os.listdir(r'/home/xuyang1/PatchTST/PatchTST_supervised/dataset/'):\n",
    "    tmp = pd.read_csv(os.path.join(r'/home/xuyang1/PatchTST/PatchTST_supervised/dataset/', name))\n",
    "    shape_series[name] = tmp.shape\n",
    "    if \"stocka\" in name:\n",
    "        tmp = tmp.set_index(\"date\")\n",
    "        tmp = tmp.rolling(20).mean()\n",
    "        fig = px.line(tmp.iloc(axis=1)[::10], title=f\"{name} Top 10% cumulative return\")\n",
    "        fig.show()\n",
    "    \n",
    "# display(shape_series.to_frame(\"dataset_size\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 验证结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "label_stocka_quintile = pd.read_csv(r'/home/xuyang1/PatchTST/PatchTST_supervised/dataset/quintile_stocka.csv', index_col=0)\n",
    "label_us_quintile = pd.read_csv(r'/home/xuyang1/PatchTST/PatchTST_supervised/dataset/quintile_us.csv', index_col=0)\n",
    "# label_stocka_quintile = pd.DataFrame(pickle.load(open(r'/home/xuyang1/PatchTST/PatchTST_supervised/dataset/quintile_stocka_transformed.pkl', 'rb')), index=label_stocka_quintile.index, columns=label_stocka_quintile.columns)\n",
    "# label_us_quintile = pd.DataFrame(pickle.load(open(r'/home/xuyang1/PatchTST/PatchTST_supervised/dataset/quintile_us_transformed.pkl', 'rb')), index=label_us_quintile.index, columns=label_us_quintile.columns)\n",
    "label_stocka_top10p = pd.read_csv(r'/home/xuyang1/PatchTST/PatchTST_supervised/dataset/top10p_stocka.csv', index_col=0)\n",
    "label_us_top10p = pd.read_csv(r'/home/xuyang1/PatchTST/PatchTST_supervised/dataset/top10p_us.csv', index_col=0)\n",
    "# label_stocka_top10p = pd.DataFrame(pickle.load(open(r'/home/xuyang1/PatchTST/PatchTST_supervised/dataset/top10p_stocka_transformed.pkl', 'rb')), index=label_stocka_top10p.index, columns=label_stocka_top10p.columns)\n",
    "# label_us_top10p = pd.DataFrame(pickle.load(open(r'/home/xuyang1/PatchTST/PatchTST_supervised/dataset/top10p_us_transformed.pkl', 'rb')), index=label_us_top10p.index, columns=label_us_top10p.columns)\n",
    "\n",
    "quintile_stocka_scaler = pickle.load(open(r'/home/xuyang1/PatchTST/PatchTST_supervised/dataset/quintile_stocka_scaler.pkl', 'rb'))\n",
    "top10p_stocka_scaler = pickle.load(open(r'/home/xuyang1/PatchTST/PatchTST_supervised/dataset/top10p_stocka_scaler.pkl', 'rb'))\n",
    "quintile_us_scaler = pickle.load(open(r'/home/xuyang1/PatchTST/PatchTST_supervised/dataset/quintile_us_scaler.pkl', 'rb'))\n",
    "top10p_us_scaler = pickle.load(open(r'/home/xuyang1/PatchTST/PatchTST_supervised/dataset/top10p_us_scaler.pkl', 'rb'))\n",
    "losses = []\n",
    "for name in os.listdir(r\"/home/xuyang1/PatchTST/PatchTST_supervised/results\"):\n",
    "    model = name.split(\"_\")[4]\n",
    "    seq_length = name.split(\"_\")[3]\n",
    "    dataset = name.split(\"_\")[0]\n",
    "    dataset_country = name.split(\"_\")[1]\n",
    "    if dataset == \"quintile\" and dataset_country == \"stocka\":\n",
    "        label = label_stocka_quintile\n",
    "    elif dataset == \"quintile\" and dataset_country == \"us\":\n",
    "        label = label_us_quintile\n",
    "    elif dataset == \"top10p\" and dataset_country == \"stocka\":\n",
    "        label = label_stocka_top10p\n",
    "    elif dataset == \"top10p\" and dataset_country == \"us\":\n",
    "        label = label_us_top10p\n",
    "    pred = np.load(rf\"/home/xuyang1/PatchTST/PatchTST_supervised/results/{name}/pred.npy\")\n",
    "\n",
    "    if dataset == \"quintile\" and dataset_country == \"stocka\":\n",
    "        scaler = quintile_stocka_scaler\n",
    "    elif dataset == \"quintile\" and dataset_country == \"us\":\n",
    "        scaler = quintile_us_scaler\n",
    "    elif dataset == \"top10p\" and dataset_country == \"stocka\":\n",
    "        scaler = top10p_stocka_scaler\n",
    "    elif dataset == \"top10p\" and dataset_country == \"us\":\n",
    "        scaler = top10p_us_scaler\n",
    "    pred = scaler.inverse_transform(np.reshape(pred, (-1, pred.shape[-1]))).reshape(pred.shape)\n",
    "\n",
    "    # if \"quinti\" not in dataset or seq_length != \"96\":\n",
    "    #     continue\n",
    "\n",
    "    test_length = int(len(label) * 0.2)\n",
    "\n",
    "    for i in range(pred.shape[-1]):\n",
    "        if label.values[:-test_length,i].mean() < 0:\n",
    "            label.iloc[:,i] = label.iloc[:,i] * -1\n",
    "            pred[:,:,i] = -pred[:,:,i]\n",
    "    target_label = label.iloc[-test_length:,:]\n",
    "    if pred.shape[0] > target_label.shape[0]:\n",
    "        pred = pred[:target_label.shape[0],:,:]\n",
    "    elif pred.shape[0] < target_label.shape[0]:\n",
    "        target_label = target_label[:pred.shape[0],:]\n",
    "\n",
    "\n",
    "    # calculate pred to real label loss:\n",
    "    # loss = 0\n",
    "    # for i in range(pred.shape[0]):\n",
    "    #     pred_batch = pred[i]\n",
    "    #     label_batch = target_label.iloc[i:i+pred.shape[1],:].values\n",
    "    #     loss += np.mean((pred_batch - label_batch)**2)\n",
    "\n",
    "    # calculate real return with prediction:\n",
    "    # 1. use closest pred as real pread\n",
    "    # real_pred = pred[:,0,:]\n",
    "\n",
    "    # 2. use mean of pred as real pred\n",
    "    # pred_middata_np = np.zeros((pred.shape[0], pred.shape[0] + pred.shape[1] - 1, pred.shape[2]))\n",
    "    # pred_middata_np[:,:,:] = np.nan\n",
    "    # for i in range(pred.shape[0]):\n",
    "    #     pred_middata_np[i, i:i+pred.shape[1], :] = pred[i]\n",
    "    # real_pred = np.nanmean(pred_middata_np, axis=0, keepdims=False)\n",
    "\n",
    "    # 3. use mean of pred as real pred, but only use the first 20 pred\n",
    "    real_pred = pred[:, :20, :].mean(axis=1, keepdims=False)\n",
    "\n",
    "    pred_df = pd.DataFrame(real_pred, index=target_label.index[:real_pred.shape[0]], columns=target_label.columns)\n",
    "\n",
    "    # ---- calculate rolling 20 days return ----\n",
    "    rolling_20_label = target_label.rolling(20).mean().shift(-19).iloc(axis=0)[:-19]\n",
    "    average_ret = rolling_20_label.mean(axis=1).to_frame(\"average_ret\")\n",
    "\n",
    "    past_rolling_20_label = target_label.rolling(20, closed=\"left\", min_periods=1).mean()\n",
    "\n",
    "    follow_past_20days_50p = rolling_20_label[(past_rolling_20_label.rank(axis=1, pct=True) > 0.5)].mean(axis=1).to_frame(\"follow_past_20days_50p\")\n",
    "    follow_past_20days_20p = rolling_20_label[(past_rolling_20_label.rank(axis=1, pct=True) > 0.8)].mean(axis=1).to_frame(\"follow_past_20days_20p\")\n",
    "    follow_past_20days_10p = rolling_20_label[(past_rolling_20_label.rank(axis=1, pct=True) > 0.9)].mean(axis=1).to_frame(\"follow_past_20days_10p\")\n",
    "\n",
    "    rolling_20_pred_df = pred_df.rolling(20).mean().shift(-20).iloc(axis=0)[:-20]\n",
    "    top50p_pred_ret = rolling_20_label[(pred_df.rank(axis=1, pct=True) > 0.5)].mean(axis=1).to_frame(\"top50p_pred_ret\")\n",
    "    top20p_pred_ret = rolling_20_label[(pred_df.rank(axis=1, pct=True) > 0.8)].mean(axis=1).to_frame(\"top20p_pred_ret\")\n",
    "    top10p_pred_ret = rolling_20_label[(pred_df.rank(axis=1, pct=True) > 0.9)].mean(axis=1).to_frame(\"top10p_pred_ret\")\n",
    "    show_df = pd.concat([average_ret, top50p_pred_ret, top20p_pred_ret, top10p_pred_ret, follow_past_20days_50p, follow_past_20days_20p, follow_past_20days_10p], axis=1).cumsum(axis=0)\n",
    "    show_df.plot()\n",
    "    plt.title(f\"{dataset_country} {dataset} {model} {seq_length}\")\n",
    "    # plt.show()\n",
    "\n",
    "    # ---- plot pred and label in same photo ----\n",
    "    # pred_with_input = pd.concat([label.iloc[:-test_length,:], pred_df], axis=0)\n",
    "    # pred_with_input.columns = [col + \"_pred\" for col in pred_with_input.columns]\n",
    "    # fig, axes = plt.subplots(5, 5)\n",
    "    # fig.set_figheight(30)\n",
    "    # fig.set_figwidth(30)\n",
    "    # plt.subplots_adjust(wspace=None, hspace=None)\n",
    "    # axes = axes.reshape(5, 5)\n",
    "    # stride = label.shape[-1] // 25\n",
    "    # for i in range(25):\n",
    "    #     pred_and_gt = pd.concat([label.iloc(axis=1)[i*stride:i*stride+1], pred_with_input.iloc(axis=1)[i*stride:i*stride+1]], axis=1).cumsum(axis=0)\n",
    "    #     pred_and_gt.plot(ax=axes[i//5, i%5], legend=True)\n",
    "    #     axes[i//5, i%5].set_title(label.columns[i*stride])\n",
    "    # plt.title(f\"{dataset_country} {dataset} {model} {seq_length}\")\n",
    "    plt.savefig(rf\"/home/xuyang1/PatchTST/PatchTST_supervised/plot_result/{dataset_country}_{dataset}_{model}_{seq_length}.png\")\n",
    "    # plt.show()\n",
    "    \n",
    "    # rics = []\n",
    "    # for col in pred_df.columns:\n",
    "    #     rics.append(pred_df[col].corr(target_label[col], method=\"spearman\"))\n",
    "\n",
    "    # print(dataset_country, dataset, seq_length, model, sum(rics) / len(rics), sep=\",\")\n",
    "    plt.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 处理美股数据\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm \n",
    "from functools import partial\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "msn_data = pd.read_parquet(r\"/home/xuyang1/PatchTST/es_data/msn_data.parq\")\n",
    "price = pd.read_parquet(r\"/home/xuyang1/PatchTST/es_data/orig_price_info.parquet\")\n",
    "\n",
    "msn_data = msn_data.iloc(axis=1)[:-1]\n",
    "msn_data.columns = [i[1] for i in msn_data.columns]\n",
    "def guess_price(df, shift, horizon):\n",
    "    return df.price.shift(-shift-horizon) / df.price.shift(-shift) - 1\n",
    "\n",
    "tqdm.pandas()\n",
    "price[\"price_1d\"] = price.groupby(\"instrument\", group_keys=False).progress_apply(partial(guess_price, shift=1, horizon=1)).sort_index()\n",
    "new_data = msn_data.join(price[\"price_1d\"], how=\"left\").dropna(subset=['price_1d'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandarallel import pandarallel\n",
    "pandarallel.initialize()\n",
    "\n",
    "tqdm.pandas()\n",
    "def calculate_quintile_ret(df):\n",
    "    label = df.iloc(axis=1)[-1]\n",
    "    quintile_ret = pd.Series(index=df.columns)\n",
    "    for col in df.columns:\n",
    "        tmp_df = pd.DataFrame({\"left\": df[col], \"right\": label}).sort_values(\"left\", ascending=False)\n",
    "        group1_ret = tmp_df.iloc[:int(len(tmp_df) * 0.2), 1].mean()\n",
    "        group5_ret = tmp_df.iloc[int(len(tmp_df) * 0.8):, 1].mean()\n",
    "        quintile_ret[col] = group1_ret - group5_ret\n",
    "\n",
    "    return quintile_ret\n",
    "\n",
    "quintile_ret_data = new_data.groupby(\"datetime\").parallel_apply(calculate_quintile_ret)\n",
    "pickle.dump(quintile_ret_data, open(r'/home/xuyang1/PatchTST/es_data/quintile_ret_data.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandarallel import pandarallel\n",
    "pandarallel.initialize()\n",
    "\n",
    "def calculate_top10p_ret(df):\n",
    "    label = df.iloc(axis=1)[-1]\n",
    "    top10p_ret = pd.Series(index=df.columns)\n",
    "    for col in df.columns:\n",
    "        tmp_df = pd.DataFrame({\"left\": df[col], \"right\": label}).sort_values(\"left\", ascending=False)\n",
    "        top10p_ret[col] = tmp_df.iloc[:int(len(tmp_df) * 0.1), 1].mean()\n",
    "    return top10p_ret\n",
    "\n",
    "top10p_ret_data = new_data.groupby(\"datetime\").parallel_apply(calculate_top10p_ret)\n",
    "pickle.dump(top10p_ret_data, open(r'/home/xuyang1/PatchTST/es_data/top10p_ret_data.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quintile_ret_data = pickle.load(open(r'/home/xuyang1/PatchTST/es_data/quintile_ret_data.pkl', 'rb'))\n",
    "top10p_ret_data = pickle.load(open(r'/home/xuyang1/PatchTST/es_data/top10p_ret_data.pkl', 'rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in tqdm(quintile_ret_data.columns):\n",
    "    count = 0\n",
    "    while True:\n",
    "        upper = quintile_ret_data[col].mean() + quintile_ret_data[col].std() * 5\n",
    "        lower = quintile_ret_data[col].mean() - quintile_ret_data[col].std() * 5\n",
    "        if (upper > quintile_ret_data[col].max()) and (lower < quintile_ret_data[col].min()):\n",
    "            break\n",
    "        if count > 3:\n",
    "            break\n",
    "        count += 1\n",
    "        quintile_ret_data[col] = quintile_ret_data[col].clip(lower, upper,)\n",
    "\n",
    "for col in tqdm(top10p_ret_data.columns):\n",
    "    count = 0\n",
    "    while True:\n",
    "        upper = top10p_ret_data[col].mean() + top10p_ret_data[col].std() * 5\n",
    "        lower = top10p_ret_data[col].mean() - top10p_ret_data[col].std() * 5\n",
    "        if (upper > top10p_ret_data[col].max()) and (lower < top10p_ret_data[col].min()):\n",
    "            break\n",
    "        if count > 3:\n",
    "            break\n",
    "        count += 1\n",
    "        top10p_ret_data[col] = top10p_ret_data[col].clip(lower, upper,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quintile_ret_data = quintile_ret_data.iloc(axis=1)[:-1]\n",
    "top10p_ret_data = top10p_ret_data.iloc(axis=1)[:-1]\n",
    "\n",
    "quintile_ret_data.index.names = [\"date\"]\n",
    "top10p_ret_data.index.names = [\"date\"]\n",
    "\n",
    "quintile_ret_data.to_csv(r'/home/xuyang1/PatchTST/PatchTST_supervised/dataset/quintile_us.csv')\n",
    "top10p_ret_data.to_csv(r'/home/xuyang1/PatchTST/PatchTST_supervised/dataset/top10p_us.csv')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 记录所有loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "lines = open(r\"/home/xuyang1/PatchTST/final_result.txt\").readlines()\n",
    "\n",
    "result_df = pd.DataFrame(columns=[\"dataset\", \"country\", \"seq_length\", \"model\", \"mse\", \"mae\", \"rse\"])\n",
    "for line in lines:\n",
    "    line = line.strip()\n",
    "    if len(line) == 0:\n",
    "        continue\n",
    "    dataset = line.split(\"_\")[0]\n",
    "    dataset_country = line.split(\"_\")[1]\n",
    "    seq_len = int(line.split(\"_\")[3])\n",
    "    model = line.split(\"_\")[4]\n",
    "    mse = float(line.split(\"mse:\")[1].split(\",\")[0])\n",
    "    mae = float(line.split(\"mae:\")[1].split(\",\")[0])\n",
    "    rse = float(line.split(\"rse:\")[1].split(\",\")[0])\n",
    "    result_df = pd.concat([result_df, pd.DataFrame([[dataset, dataset_country, seq_len, model, mse, mae, rse]], columns=result_df.columns)], axis=0)\n",
    "\n",
    "result_df = result_df.set_index([\"dataset\", \"country\", \"seq_length\", \"model\"]).sort_index()\n",
    "result_df.to_excel(r\"/home/xuyang1/PatchTST/final_result.xlsx\")\n",
    "# display(result_df)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 拼接所有图片"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "file_names = os.listdir(r\"/home/xuyang1/PatchTST/PatchTST_supervised/plot_result\")\n",
    "\n",
    "result_df = pd.DataFrame(columns=[\"dataset\", \"country\", \"seq_length\", \"model\", \"name\"])\n",
    "for original_name in file_names:\n",
    "    name = original_name.split(\".\")[0]\n",
    "    dataset_country = name.split(\"_\")[0]\n",
    "    dataset = name.split(\"_\")[1]\n",
    "    model = name.split(\"_\")[2]\n",
    "    seq_len = int(name.split(\"_\")[3])\n",
    "    \n",
    "    result_df = pd.concat([result_df, pd.DataFrame([[dataset, dataset_country, seq_len, model, f\"/home/xuyang1/PatchTST/PatchTST_supervised/plot_result/{original_name}\"]], columns=result_df.columns)], axis=0)\n",
    "\n",
    "\n",
    "\n",
    "result_df = result_df.set_index([\"dataset\", \"country\", \"model\", \"seq_length\", ]).sort_index()\n",
    "# result_df = result_df.set_index([\"model\"])\n",
    "for dataset in result_df.index.get_level_values(\"dataset\").drop_duplicates():\n",
    "    part_df1 = result_df.loc[dataset]\n",
    "    for country in part_df1.index.get_level_values(\"country\").drop_duplicates():\n",
    "        part_df2 = part_df1.loc[country]\n",
    "        images = []\n",
    "        for model in part_df2.index.get_level_values(\"model\").drop_duplicates():\n",
    "            part_df3 = part_df2.loc[model]\n",
    "            for seq_length in part_df3.index.get_level_values(\"seq_length\").drop_duplicates():\n",
    "                part_df4 = part_df3.loc(axis=0)[seq_length]\n",
    "                img = cv2.imread(part_df4[\"name\"])\n",
    "                cv2.putText(img, f\"{dataset}_{country}_{seq_length}_{model}\", (0, 100), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "                images.append(img)\n",
    "        height, width, _ = images[0].shape  \n",
    "        row_num = len(images) // 4\n",
    "        col_num = 4\n",
    "    \n",
    "        # 创建一个空白图片，用于存放拼接后的图片  \n",
    "        result_image = np.zeros((height * row_num, width * col_num, 3), np.uint8)  \n",
    "    \n",
    "        # 遍历每个图片，按照九宫格的顺序将其粘贴到结果图片上  \n",
    "        for i, image in enumerate(images):  \n",
    "            x = i % col_num * width  \n",
    "            y = i // col_num * height  \n",
    "            result_image[y:y+height, x:x+width] = image\n",
    "        result_image = cv2.resize(result_image, (result_image.shape[1], result_image.shape[0]))\n",
    "        cv2.imwrite(f'/home/xuyang1/PatchTST/PatchTST_supervised/plot_result_combined/{dataset}_{country}.png', result_image,)\n",
    "\n",
    "\n",
    "\n",
    "display(result_df)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 对比 self supervised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "label_stocka_quintile = pd.read_csv(r'/home/xuyang1/PatchTST/PatchTST_supervised/dataset/quintile_stocka.csv', index_col=0)\n",
    "label_us_quintile = pd.read_csv(r'/home/xuyang1/PatchTST/PatchTST_supervised/dataset/quintile_us.csv', index_col=0)\n",
    "# label_stocka_quintile = pd.DataFrame(pickle.load(open(r'/home/xuyang1/PatchTST/PatchTST_supervised/dataset/quintile_stocka_transformed.pkl', 'rb')), index=label_stocka_quintile.index, columns=label_stocka_quintile.columns)\n",
    "# label_us_quintile = pd.DataFrame(pickle.load(open(r'/home/xuyang1/PatchTST/PatchTST_supervised/dataset/quintile_us_transformed.pkl', 'rb')), index=label_us_quintile.index, columns=label_us_quintile.columns)\n",
    "label_stocka_top10p = pd.read_csv(r'/home/xuyang1/PatchTST/PatchTST_supervised/dataset/top10p_stocka.csv', index_col=0)\n",
    "label_us_top10p = pd.read_csv(r'/home/xuyang1/PatchTST/PatchTST_supervised/dataset/top10p_us.csv', index_col=0)\n",
    "# label_stocka_top10p = pd.DataFrame(pickle.load(open(r'/home/xuyang1/PatchTST/PatchTST_supervised/dataset/top10p_stocka_transformed.pkl', 'rb')), index=label_stocka_top10p.index, columns=label_stocka_top10p.columns)\n",
    "# label_us_top10p = pd.DataFrame(pickle.load(open(r'/home/xuyang1/PatchTST/PatchTST_supervised/dataset/top10p_us_transformed.pkl', 'rb')), index=label_us_top10p.index, columns=label_us_top10p.columns)\n",
    "\n",
    "quintile_stocka_scaler = pickle.load(open(r'/home/xuyang1/PatchTST/PatchTST_supervised/dataset/quintile_stocka_scaler.pkl', 'rb'))\n",
    "top10p_stocka_scaler = pickle.load(open(r'/home/xuyang1/PatchTST/PatchTST_supervised/dataset/top10p_stocka_scaler.pkl', 'rb'))\n",
    "quintile_us_scaler = pickle.load(open(r'/home/xuyang1/PatchTST/PatchTST_supervised/dataset/quintile_us_scaler.pkl', 'rb'))\n",
    "top10p_us_scaler = pickle.load(open(r'/home/xuyang1/PatchTST/PatchTST_supervised/dataset/top10p_us_scaler.pkl', 'rb'))\n",
    "losses = []\n",
    "for name in os.listdir(r\"/home/xuyang1/PatchTST/PatchTST_self_supervised/results\"):\n",
    "    if \"finetune\" in name or \"linear-probe\" in name:\n",
    "        model = \"PatchTST\" + name.split(\"_\")[3]\n",
    "        seq_length = name.split(\"tw\")[-1].split(\"_\")[0]\n",
    "        dataset = name.split(\"_\")[0]\n",
    "        dataset_country = name.split(\"_\")[1]\n",
    "    else:\n",
    "        model = name.split(\"_\")[4]\n",
    "        seq_length = name.split(\"_\")[3]\n",
    "        dataset = name.split(\"_\")[0]\n",
    "        dataset_country = name.split(\"_\")[1]\n",
    "    if dataset == \"quintile\" and dataset_country == \"stocka\":\n",
    "        label = label_stocka_quintile\n",
    "    elif dataset == \"quintile\" and dataset_country == \"us\":\n",
    "        label = label_us_quintile\n",
    "    elif dataset == \"top10p\" and dataset_country == \"stocka\":\n",
    "        label = label_stocka_top10p\n",
    "    elif dataset == \"top10p\" and dataset_country == \"us\":\n",
    "        label = label_us_top10p\n",
    "    pred = np.load(rf\"/home/xuyang1/PatchTST/PatchTST_self_supervised/results/{name}/pred.npy\")\n",
    "    # if \"stocka\" == dataset_country:\n",
    "    #     pred = pred[:593, :, :]\n",
    "    # else:\n",
    "    #     pred = pred[:986, :, :]\n",
    "\n",
    "    if dataset == \"quintile\" and dataset_country == \"stocka\":\n",
    "        scaler = quintile_stocka_scaler\n",
    "    elif dataset == \"quintile\" and dataset_country == \"us\":\n",
    "        scaler = quintile_us_scaler\n",
    "    elif dataset == \"top10p\" and dataset_country == \"stocka\":\n",
    "        scaler = top10p_stocka_scaler\n",
    "    elif dataset == \"top10p\" and dataset_country == \"us\":\n",
    "        scaler = top10p_us_scaler\n",
    "    pred = scaler.inverse_transform(np.reshape(pred, (-1, pred.shape[-1]))).reshape(pred.shape)\n",
    "\n",
    "    # if \"quinti\" not in dataset or seq_length != \"96\":\n",
    "    #     continue\n",
    "\n",
    "    test_length = int(len(label) * 0.2)\n",
    "\n",
    "    for i in range(pred.shape[-1]):\n",
    "        if label.values[:-test_length,i].mean() < 0:\n",
    "            label.iloc[:,i] = label.iloc[:,i] * -1\n",
    "            pred[:,:,i] = -pred[:,:,i]\n",
    "    target_label = label.iloc[-test_length:,:]\n",
    "    if pred.shape[0] > target_label.shape[0]:\n",
    "        pred = pred[:target_label.shape[0],:,:]\n",
    "    elif pred.shape[0] < target_label.shape[0]:\n",
    "        target_label = target_label.iloc[:pred.shape[0],:]\n",
    "\n",
    "\n",
    "    # calculate pred to real label loss:\n",
    "    loss = 0\n",
    "    for i in range(pred.shape[0]):\n",
    "        pred_batch = pred[i]\n",
    "        label_batch = target_label.iloc[i:i+pred.shape[1],:].values\n",
    "        loss += np.mean((pred_batch - label_batch)**2)\n",
    "    mse = np.mean((pred[:,0,:] - target_label.values)**2)\n",
    "    mae = np.mean(np.abs(pred[:,0,:] - target_label.values))\n",
    "    print(f\"{model},{dataset},{dataset_country},{seq_length},{mse},{mae}\")\n",
    "    continue\n",
    "\n",
    "    # calculate real return with prediction:\n",
    "    # 1. use closest pred as real pread\n",
    "    # real_pred = pred[:,0,:]\n",
    "\n",
    "    # 2. use mean of pred as real pred\n",
    "    # pred_middata_np = np.zeros((pred.shape[0], pred.shape[0] + pred.shape[1] - 1, pred.shape[2]))\n",
    "    # pred_middata_np[:,:,:] = np.nan\n",
    "    # for i in range(pred.shape[0]):\n",
    "    #     pred_middata_np[i, i:i+pred.shape[1], :] = pred[i]\n",
    "    # real_pred = np.nanmean(pred_middata_np, axis=0, keepdims=False)\n",
    "\n",
    "    # 3. use mean of pred as real pred, but only use the first 20 pred\n",
    "    # real_pred = pred[:, :20, :].mean(axis=1, keepdims=False)\n",
    "\n",
    "    # pred_df = pd.DataFrame(real_pred, index=target_label.index[:real_pred.shape[0]], columns=target_label.columns)\n",
    "\n",
    "    # # ---- calculate rolling 20 days return ----\n",
    "    # rolling_20_label = target_label.rolling(20).mean().shift(-19).iloc(axis=0)[:-19]\n",
    "    # average_ret = rolling_20_label.mean(axis=1).to_frame(\"average_ret\")\n",
    "\n",
    "    # past_rolling_20_label = target_label.rolling(20, closed=\"left\", min_periods=1).mean()\n",
    "\n",
    "    # follow_past_20days_50p = rolling_20_label[(past_rolling_20_label.rank(axis=1, pct=True) > 0.5)].mean(axis=1).to_frame(\"follow_past_20days_50p\")\n",
    "    # follow_past_20days_20p = rolling_20_label[(past_rolling_20_label.rank(axis=1, pct=True) > 0.8)].mean(axis=1).to_frame(\"follow_past_20days_20p\")\n",
    "    # follow_past_20days_10p = rolling_20_label[(past_rolling_20_label.rank(axis=1, pct=True) > 0.9)].mean(axis=1).to_frame(\"follow_past_20days_10p\")\n",
    "\n",
    "    # rolling_20_pred_df = pred_df.rolling(20).mean().shift(-20).iloc(axis=0)[:-20]\n",
    "    # top50p_pred_ret = rolling_20_label[(pred_df.rank(axis=1, pct=True) > 0.5)].mean(axis=1).to_frame(\"top50p_pred_ret\")\n",
    "    # top20p_pred_ret = rolling_20_label[(pred_df.rank(axis=1, pct=True) > 0.8)].mean(axis=1).to_frame(\"top20p_pred_ret\")\n",
    "    # top10p_pred_ret = rolling_20_label[(pred_df.rank(axis=1, pct=True) > 0.9)].mean(axis=1).to_frame(\"top10p_pred_ret\")\n",
    "    # show_df = pd.concat([average_ret, top50p_pred_ret, top20p_pred_ret, top10p_pred_ret, follow_past_20days_50p, follow_past_20days_20p, follow_past_20days_10p], axis=1).cumsum(axis=0)\n",
    "    # show_df.plot()\n",
    "    # plt.title(f\"{dataset_country} {dataset} {model} {seq_length}\")\n",
    "    # plt.show()\n",
    "\n",
    "    # ---- plot pred and label in same photo ----\n",
    "    # pred_with_input = pd.concat([label.iloc[:-test_length,:], pred_df], axis=0)\n",
    "    # pred_with_input.columns = [col + \"_pred\" for col in pred_with_input.columns]\n",
    "    # fig, axes = plt.subplots(5, 5)\n",
    "    # fig.set_figheight(30)\n",
    "    # fig.set_figwidth(30)\n",
    "    # plt.subplots_adjust(wspace=None, hspace=None)\n",
    "    # axes = axes.reshape(5, 5)\n",
    "    # stride = label.shape[-1] // 25\n",
    "    # for i in range(25):\n",
    "    #     pred_and_gt = pd.concat([label.iloc(axis=1)[i*stride:i*stride+1], pred_with_input.iloc(axis=1)[i*stride:i*stride+1]], axis=1).cumsum(axis=0)\n",
    "    #     pred_and_gt.plot(ax=axes[i//5, i%5], legend=True)\n",
    "    #     axes[i//5, i%5].set_title(label.columns[i*stride])\n",
    "    # plt.title(f\"{dataset_country} {dataset} {model} {seq_length}\")\n",
    "    # plt.savefig(rf\"/home/xuyang1/PatchTST/PatchTST_self_supervised/plot_result/{dataset_country}_{dataset}_{model}_{seq_length}.png\")\n",
    "    # plt.show()\n",
    "    \n",
    "    # rics = []\n",
    "    # for col in pred_df.columns:\n",
    "    #     rics.append(pred_df[col].corr(target_label[col], method=\"spearman\"))\n",
    "\n",
    "    # print(dataset_country, dataset, seq_length, model, sum(rics) / len(rics), sep=\",\")\n",
    "    plt.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 对比 fintune and supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  \n",
    "  \n",
    "def r_squared(y_true, y_pred):  \n",
    "    \"\"\"  \n",
    "    Calculate the R-squared (coefficient of determination) for given true and predicted values.  \n",
    "  \n",
    "    :param y_true: list or numpy array of true values  \n",
    "    :param y_pred: list or numpy array of predicted values  \n",
    "    :return: R-squared value  \n",
    "    \"\"\"  \n",
    "    y_true = np.array(y_true)  \n",
    "    y_pred = np.array(y_pred)  \n",
    "  \n",
    "    # Calculate the mean of true values  \n",
    "    y_mean = np.mean(y_true)  \n",
    "  \n",
    "    # Calculate the total sum of squares (TSS)  \n",
    "    tss = np.sum((y_true - y_mean) ** 2)  \n",
    "  \n",
    "    # Calculate the residual sum of squares (RSS)  \n",
    "    rss = np.sum((y_true - y_pred) ** 2)  \n",
    "  \n",
    "    # Calculate R-squared  \n",
    "    r2 = 1 - (rss / tss)  \n",
    "  \n",
    "    return r2  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 对比 ff data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "label_us_quintile = pd.read_csv(r'/home/xuyang1/PatchTST/PatchTST_supervised/dataset/ff_data.csv', index_col=0)\n",
    "# label_us_quintile = pd.DataFrame(pickle.load(open(r'/home/xuyang1/PatchTST/PatchTST_supervised/dataset/ff_data_transformed.pkl', 'rb')), index=label_us_quintile.index, columns=label_us_quintile.columns)\n",
    "\n",
    "pred_supervised = np.load(r\"/home/xuyang1/PatchTST/PatchTST_supervised/results/ff_data_512_96_PatchTST_custom_ftM_sl512_ll48_pl96_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_Exp_0/pred.npy\")\n",
    "\n",
    "pred_linear_prob = np.load(r\"/home/xuyang1/PatchTST/PatchTST_self_supervised/saved_models/ff_data/masked_patchtst/based_model/ff_data_patchtst_linear-probe_cw512_tw96_patch16_stride8_epochs-finetune100_model1/pred.npy\")\n",
    "\n",
    "pred_finetune = np.load(r\"/home/xuyang1/PatchTST/PatchTST_self_supervised/saved_models/ff_data/masked_patchtst/based_model/ff_data_patchtst_finetuned_cw512_tw96_patch16_stride8_epochs-finetune100_model1/pred.npy\")\n",
    "\n",
    "label = label_us_quintile\n",
    "test_length = int(len(label) * 0.2)\n",
    "\n",
    "\n",
    "target_label = label.iloc[-test_length:,:]\n",
    "pred_supervised = pred_supervised[:4997,:,:]\n",
    "pred_linear_prob = pred_linear_prob[:4997,:,:]\n",
    "pred_finetune = pred_finetune[:4997,:,:]\n",
    "ff_scaler = pickle.load(open(r'/home/xuyang1/PatchTST/PatchTST_supervised/dataset/ff_data_scaler.pkl', 'rb'))\n",
    "pred_supervised = ff_scaler.inverse_transform(np.reshape(pred_supervised, (-1, pred_supervised.shape[-1]))).reshape(pred_supervised.shape)\n",
    "pred_linear_prob = ff_scaler.inverse_transform(np.reshape(pred_linear_prob, (-1, pred_linear_prob.shape[-1]))).reshape(pred_linear_prob.shape)\n",
    "pred_finetune = ff_scaler.inverse_transform(np.reshape(pred_finetune, (-1, pred_finetune.shape[-1]))).reshape(pred_finetune.shape)\n",
    "\n",
    "# pred_supervised = np.reshape(pred_supervised[:4997,:], (-1, 3))\n",
    "# pred_linear_prob = np.reshape(pred_linear_prob[:4997,:], (-1, 3))\n",
    "# pred_finetune = np.reshape(pred_finetune[:4997,:], (-1, 3))\n",
    "\n",
    "# display(test_length, target_label.shape, pred_supervised.shape, pred_linear_prob.shape)\n",
    "# target_label_list = [target_label[i:i+96] for i in range(4997)]\n",
    "# target_label_aligned = np.concatenate(target_label_list, axis=0)\n",
    "\n",
    "# loss_supervised = np.mean((target_label_aligned - pred_supervised)**2)\n",
    "# loss_linear_prob = np.mean((target_label_aligned - pred_linear_prob)**2)\n",
    "# loss_finetune = np.mean((target_label_aligned - pred_finetune)**2)\n",
    "\n",
    "# r2_supervised = r_squared(target_label_aligned, pred_supervised)\n",
    "# r2_linear_prob = r_squared(target_label_aligned, pred_linear_prob)\n",
    "# r2_finetune = r_squared(target_label_aligned, pred_finetune)\n",
    "\n",
    "# print(\"loss_supervised:\", loss_supervised, \"loss_linear_prob:\", loss_linear_prob, \"loss_finetune:\", loss_finetune)\n",
    "# print(\"r2_supervised:\", r2_supervised, \"r2_linear_prob:\", r2_linear_prob, \"r2_finetune:\", r2_finetune)\n",
    "show_df = []\n",
    "for pred in [pred_supervised, pred_linear_prob, pred_finetune]:\n",
    "    \n",
    "    if pred is pred_supervised:\n",
    "        title = \"supervised\"\n",
    "    elif pred is pred_linear_prob:\n",
    "        title = \"linear_prob\"\n",
    "    elif pred is pred_finetune:\n",
    "        title = \"finetune\"\n",
    "    real_pred = pred[:, :20, :].mean(axis=1, keepdims=False)\n",
    "                \n",
    "    pred_df = pd.DataFrame(real_pred, index=target_label.index[:real_pred.shape[0]], columns=target_label.columns)\n",
    "\n",
    "    # rolling_20_label = target_label.rolling(20).mean().shift(-19).iloc(axis=0)[:-19] / 30\n",
    "    average_ret = target_label.mean(axis=1).to_frame(\"average_ret\")\n",
    "\n",
    "    past_rolling_20_label = target_label.rolling(20, closed=\"left\", min_periods=1).mean()\n",
    "    # past_rolling_20_label = target_label.shift(30)\n",
    "\n",
    "    display(past_rolling_20_label.shape, target_label.shape)\n",
    "\n",
    "    \n",
    "    factor1 = target_label.iloc[:,0].to_frame(f\"factor1\")\n",
    "    factor2 = target_label.iloc[:,1].to_frame(f\"factor2\")\n",
    "    factor3 = target_label.iloc[:,2].to_frame(f\"factor3\")\n",
    "\n",
    "    display(past_rolling_20_label.head(), target_label.head(), past_rolling_20_label.rank(axis=1).head())\n",
    "\n",
    "    follow_past_20days_50p = target_label[(past_rolling_20_label.rank(axis=1) >= 2)].mean(axis=1).to_frame(f\"{title}_follow_past_20days_top2\")\n",
    "    follow_past_20days_20p = target_label[(past_rolling_20_label.rank(axis=1) >= 3)].mean(axis=1).to_frame(f\"{title}_follow_past_20days_top1\")\n",
    "\n",
    "    # top50p_pred_ret = target_label[(pred_df.rank(axis=1) >= 2)].mean(axis=1).to_frame(f\"{title}_top1_pred_ret\")\n",
    "    # top20p_pred_ret = target_label[(pred_df.rank(axis=1) >= 3)].mean(axis=1).to_frame(f\"{title}_top2_pred_ret\")\n",
    "    show_df = pd.concat([average_ret, factor1, factor2, factor3, follow_past_20days_50p, follow_past_20days_20p], axis=1)\n",
    "    # show_df = pd.concat([average_ret, top50p_pred_ret, top20p_pred_ret, follow_past_20days_50p, follow_past_20days_20p, factor1, factor2, factor3], axis=1)\n",
    "    show_df = show_df.iloc[30:,:]\n",
    "    show_df = show_df.cumsum(axis=0)\n",
    "    # show_df.extend([average_ret, top20p_pred_ret])\n",
    "\n",
    "# show_df = pd.concat(show_df, axis=1).cumsum(axis=0)\n",
    "# show_df = show_df.iloc(axis=1)[[0,1,3,5]]\n",
    "    show_df.plot()\n",
    "    # if pred is pred_supervised:\n",
    "    #     plt.title(\"supervised\")\n",
    "    # elif pred is pred_linear_prob:\n",
    "    #     plt.title(\"linear_prob\")\n",
    "    # elif pred is pred_finetune:\n",
    "    #     plt.title(\"finetune\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "ff_data = pd.read_csv(r\"/home/xuyang1/PatchTST/PatchTST_supervised/dataset/F-F_Research_Data_Factors_daily.CSV\")\n",
    "\n",
    "ff_data.columns = [\"Date\", \"Mkt-RF\", \"SMB\", \"HML\", \"RF\"]\n",
    "ff_data[\"Date\"] = pd.to_datetime(ff_data[\"Date\"], format=\"%Y%m%d\")\n",
    "ff_data.drop(\"RF\", axis=1, inplace=True) # Drop the date \n",
    "ff_data.set_index(\"Date\", inplace=True)\n",
    "\n",
    "display(ff_data.head())\n",
    "\n",
    "ff_data.to_csv(r'/home/xuyang1/PatchTST/PatchTST_self_supervised/data/datasets/ff_data.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 处理一分钟数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 24 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "from pandarallel import pandarallel\n",
    "pandarallel.initialize()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "csi500_instrument = pd.read_csv(r\"/home/xuyang1/PatchTST/csi500.txt\", header=None).set_index(0)\n",
    "csi500_instrument.columns = [\"start_time\", \"end_time\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train (17040960, 6) (574916400, 6)\n",
      "valid (2866800, 6) (101622480, 6)\n",
      "test (2524080, 6) (92534640, 6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Bad pipe message: %s [b'GR\\xf6V>\\xd0\\xdb\\x9f8J\\xf9\\xa8\\x96\\xeb\\xc7\\xca\\x1c\\x87\\x00\\x00>\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\xc0\\x0f\\xc0\\x05\\x005\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00\\x96\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\x00\\xff\\x02\\x01\\x00\\x00C\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x1c\\x00', b'\\x17\\x00\\x19\\x00\\x1c\\x00\\x1b\\x00\\x18\\x00\\x1a\\x00\\x16\\x00\\x0e\\x00\\r\\x00\\x0b\\x00\\x0c\\x00\\t\\x00\\n']\n",
      "Bad pipe message: %s [b'\\xadQxG*\\x08>5h\\x97Cy<\\xf3\\x843}\\xec\\x00\\x00\\xa2', b'\\xc0\\n\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\\x19\\x00:\\x00\\x89\\xc0\\x0f\\xc0\\x05\\x005\\x00\\x84\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00E\\x00D\\x00C\\x00B\\xc0\\x18\\x004\\x00\\x9b\\x00F\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x96\\x00A\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x16\\x00\\x18\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\xc0\\x12\\xc0\\x08\\x00\\x16\\x00\\x13\\x00\\x10\\x00\\r\\xc0\\x17\\x00\\x1b\\xc0\\r\\xc0\\x03\\x00\\n\\x00\\x15\\x00\\x12\\x00\\x0f\\x00\\x0c\\x00\\x1a\\x00\\t\\x00\\x14\\x00\\x11\\x00\\x19\\x00\\x08\\x00\\x06\\x00\\x17\\x00\\x03\\xc0\\x10\\xc0\\x06\\xc0\\x15\\xc0\\x0b\\xc0\\x01\\x00\\x02\\x00\\x01\\x00\\xff\\x02\\x01\\x00\\x00C\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02']\n",
      "Bad pipe message: %s [b'U\\t\\xb3\\xb1J\\xac\\xea$w\\xcf\\x12q\\x0e\\x13H\\x87\\xd6:\\x00\\x00\\xa2\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\\x19\\x00:\\x00\\x89\\xc0\\x0f\\xc0\\x05\\x005\\x00\\x84\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00E\\x00D\\x00C\\x00B\\xc0\\x18\\x004\\x00\\x9b\\x00F\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x96\\x00A\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x16\\x00\\x18\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\xc0\\x12\\xc0\\x08\\x00\\x16\\x00\\x13\\x00\\x10\\x00\\r\\xc0\\x17\\x00\\x1b\\xc0\\r\\xc0\\x03\\x00\\n\\x00\\x15\\x00\\x12\\x00\\x0f\\x00\\x0c\\x00\\x1a\\x00\\t\\x00\\x14\\x00\\x11\\x00\\x19\\x00\\x08\\x00\\x06\\x00\\x17\\x00\\x03\\xc0\\x10\\xc0\\x06\\xc0\\x15\\xc0\\x0b\\xc0\\x01\\x00\\x02\\x00\\x01\\x00\\xff\\x02\\x01\\x00\\x00C\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0']\n",
      "Bad pipe message: %s [b' \\x1c.\\x1e\\x0c\\x9b\\xe4\\xea\\xf6O\\xda\\x9e\\xc6L\\xc7+\\xceN\\x00\\x00\\xa2\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\\x19\\x00:\\x00\\x89\\xc0\\x0f\\xc0\\x05\\x005\\x00\\x84\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00E\\x00D\\x00C\\x00B\\xc0\\x18\\x004\\x00\\x9b\\x00F\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x96\\x00A\\x00\\x07\\xc0']\n",
      "Bad pipe message: %s [b'\\x07\\xc0\\x16\\x00\\x18\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\xc0\\x12\\xc0']\n",
      "Bad pipe message: %s [b'\\x16\\x00\\x13\\x00\\x10\\x00\\r']\n",
      "Bad pipe message: %s [b\"\\x05\\xae\\x13\\x04U\\xfc\\x1c\\xa8\\xcdSm\\xb6Z\\xa8\\xf2b+\\x1b\\x00\\x00\\x86\\xc00\\xc0,\\xc0(\\xc0$\\xc0\\x14\\xc0\\n\\x00\\xa5\\x00\\xa3\\x00\\xa1\\x00\\x9f\\x00k\\x00j\\x00i\\x00h\\x009\\x008\\x007\\x006\\xc02\\xc0.\\xc0*\\xc0&\\xc0\\x0f\\xc0\\x05\\x00\\x9d\\x00=\\x005\\xc0/\\xc0+\\xc0'\\xc0#\"]\n",
      "Bad pipe message: %s [b\" \\r\\x0fI\\xe40\\xd9\\xbc\\xe2\\xe7\\xab\\x8b\\r\\xfe\\xf0\\x15\\xf4\\xb5\\x00\\x00\\xf4\\xc00\\xc0,\\xc0(\\xc0$\\xc0\\x14\\xc0\\n\\x00\\xa5\\x00\\xa3\\x00\\xa1\\x00\\x9f\\x00k\\x00j\\x00i\\x00h\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\\x19\\x00\\xa7\\x00m\\x00:\\x00\\x89\\xc02\\xc0.\\xc0*\\xc0&\\xc0\\x0f\\xc0\\x05\\x00\\x9d\\x00=\\x005\\x00\\x84\\xc0/\\xc0+\\xc0'\\xc0#\\xc0\\x13\\xc0\\t\\x00\\xa4\"]\n",
      "Bad pipe message: %s [b'\\xb9\\n\\x81%\\xa5LJ\\x1bw8\\x03\\x1d\\xfa\\x90\\x05\\xc5p\\x97 \\xa1v5\\xa3%I\\xc6']\n",
      "Bad pipe message: %s [b'\\xf4<\\x0e\\xa3\\x9fV\\xe3\\x8a5\\xc0!?$\\xb3Q3\\x8a(\\xc3']\n",
      "Bad pipe message: %s [b'D\\x08U\\x03\\xaa\\xc0Y}6\\xc1\\xad\\xb9\\xb7\\x825T\\t\\x12 \\xbb\\x9c@\\x8d8\\xf1\\xe3\\x9b\\x99\\xea\\xfex\\xac\\xb9\\x86\\xb1A\\x8d\\xd0\\x07PG\\x8d\\xf1\\x0e\\xf9\\x1aj\\xca\\x90\\xcc;\\x00\\x08\\x13\\x02\\x13\\x03\\x13\\x01\\x00\\xff\\x01\\x00\\x00\\x8f\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x0c\\x00\\n\\x00\\x1d\\x00\\x17\\x00\\x1e\\x00\\x19\\x00\\x18\\x00#\\x00\\x00']\n",
      "Bad pipe message: %s [b'/\\x13\\x7f\\xe2\\xed\\x15\\x1e,\\xf5\\xa5\\x7f\\xe4\\xe1\\xa695\\x89\\xa0\\x00\\x00|\\xc0,\\xc00\\x00\\xa3\\x00']\n",
      "Bad pipe message: %s [b\"\\xa9\\xcc\\xa8\\xcc\\xaa\\xc0\\xaf\\xc0\\xad\\xc0\\xa3\\xc0\\x9f\\xc0]\\xc0a\\xc0W\\xc0S\\xc0+\\xc0/\\x00\\xa2\\x00\\x9e\\xc0\\xae\\xc0\\xac\\xc0\\xa2\\xc0\\x9e\\xc0\\\\\\xc0`\\xc0V\\xc0R\\xc0$\\xc0(\\x00k\\x00j\\xc0#\\xc0'\\x00g\\x00@\\xc0\\n\\xc0\\x14\\x009\\x008\\xc0\\t\\xc0\\x13\\x003\\x002\\x00\\x9d\\xc0\\xa1\\xc0\\x9d\\xc0Q\\x00\\x9c\\xc0\\xa0\\xc0\\x9c\\xc0P\\x00=\\x00<\\x005\\x00/\\x00\\x9a\\x00\\x99\\xc0\\x07\\xc0\\x11\\x00\\x96\\x00\\x05\\x00\\xff\\x01\\x00\\x00j\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x0c\\x00\\n\\x00\\x1d\\x00\\x17\\x00\\x1e\\x00\"]\n",
      "Bad pipe message: %s [b'\\x18\\x00#\\x00\\x00\\x00\\x16\\x00\\x00\\x00\\x17\\x00\\x00\\x00\\r\\x000\\x00.\\x04\\x03\\x05\\x03\\x06']\n",
      "Bad pipe message: %s [b'\\x07\\x08']\n",
      "Bad pipe message: %s [b'\\t\\x08\\n\\x08\\x0b\\x08\\x04']\n",
      "Bad pipe message: %s [b'\\x08\\x06\\x04\\x01\\x05\\x01\\x06', b'', b'\\x03\\x03']\n",
      "Bad pipe message: %s [b'']\n",
      "Bad pipe message: %s [b'', b'\\x02']\n",
      "Bad pipe message: %s [b'\\xa4\\x88L\\xf9\\xc4\\xcd\\xce\\xd2\\xb6\\xbb\\xdf\\x13\\xef\\xaeOGrm\\x00\\x00>\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\xc0\\x0f\\xc0\\x05\\x005\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00\\x96\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\x00\\xff\\x02\\x01\\x00\\x00C\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x1c\\x00\\x1a\\x00\\x17\\x00\\x19\\x00\\x1c\\x00\\x1b\\x00\\x18\\x00\\x1a\\x00\\x16\\x00\\x0e\\x00\\r\\x00\\x0b\\x00\\x0c\\x00\\t\\x00\\n\\x00#\\x00\\x00\\x00\\x0f\\x00\\x01\\x01\\x15\\x03', b'\\x02']\n",
      "Bad pipe message: %s [b'\\x05\\x02\\x06']\n",
      "Bad pipe message: %s [b'\\x8d\\x8f\\x87O\\xab_\\xec\\x87\\xb0\\xcf\\xb3\\xf7$\\xfb}\\xe2}h\\x00\\x00\\xa2\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\\x19\\x00:\\x00\\x89\\xc0\\x0f\\xc0\\x05\\x005\\x00\\x84\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\x00\\x9a\\x00', b'\\x98\\x00\\x97\\x00E\\x00D\\x00C\\x00B\\xc0\\x18\\x004\\x00\\x9b\\x00F\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x96\\x00A\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x16\\x00\\x18\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\xc0\\x12\\xc0\\x08\\x00\\x16\\x00\\x13\\x00\\x10\\x00\\r\\xc0\\x17\\x00\\x1b\\xc0\\r\\xc0\\x03\\x00\\n\\x00\\x15\\x00\\x12\\x00\\x0f\\x00\\x0c\\x00\\x1a\\x00\\t\\x00\\x14\\x00\\x11\\x00\\x19\\x00\\x08\\x00\\x06\\x00\\x17\\x00\\x03\\xc0\\x10\\xc0\\x06\\xc0\\x15\\xc0\\x0b\\xc0\\x01\\x00\\x02\\x00\\x01\\x00\\xff\\x02\\x01\\x00\\x00C\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x1c\\x00\\x1a\\x00\\x17\\x00\\x19']\n",
      "Bad pipe message: %s [b'\\xa3\\x9c\\x03\\xd7\\xe9h\\x94\\xb3K2\\xf5R2&\\x96a\\xef!\\x00\\x00>\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\xc0\\x0f\\xc0\\x05\\x005\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00\\x96\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\x00\\xff\\x02\\x01\\x00\\x00C\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x1c\\x00', b'\\x17\\x00\\x19\\x00\\x1c\\x00\\x1b\\x00\\x18\\x00\\x1a\\x00\\x16\\x00\\x0e\\x00\\r\\x00\\x0b\\x00\\x0c\\x00\\t\\x00\\n']\n",
      "Bad pipe message: %s [b\"7\\x98\\x0f^x\\xce'>\\x14L\\xb3F{\\xc3\\x17\\x07@\\xd1\\x00\\x00\\xa2\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\\x19\\x00:\\x00\\x89\\xc0\\x0f\\xc0\\x05\\x005\\x00\\x84\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00E\\x00D\\x00C\\x00B\\xc0\\x18\\x004\\x00\\x9b\\x00F\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x96\\x00A\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x16\\x00\\x18\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\xc0\\x12\\xc0\\x08\\x00\\x16\\x00\\x13\\x00\\x10\\x00\\r\\xc0\\x17\\x00\\x1b\\xc0\\r\\xc0\\x03\\x00\\n\\x00\\x15\\x00\\x12\\x00\\x0f\\x00\\x0c\\x00\\x1a\\x00\\t\\x00\\x14\\x00\\x11\\x00\\x19\\x00\\x08\\x00\\x06\\x00\\x17\\x00\\x03\\xc0\\x10\\xc0\\x06\\xc0\\x15\\xc0\\x0b\\xc0\\x01\\x00\\x02\\x00\\x01\\x00\\xff\\x02\\x01\\x00\\x00C\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\"]\n",
      "Bad pipe message: %s [b'']\n",
      "Bad pipe message: %s [b'\\x1ay\\xad\\xee@K\\xf8\\x7fZ\\xa0\\xe6(\\x9cV\\x9fI\\xeb\\t\\x00\\x00\\x86\\xc00\\xc0,\\xc0(\\xc0$\\xc0\\x14\\xc0\\n\\x00\\xa5\\x00\\xa3\\x00\\xa1\\x00\\x9f\\x00k\\x00j\\x00i\\x00h\\x009\\x008\\x007\\x006\\xc02\\xc0.\\xc0*\\xc0&\\xc0\\x0f\\xc0\\x05\\x00\\x9d\\x00=\\x005\\xc0/\\xc0', b\"'\\xc0#\\xc0\\x13\\xc0\\t\\x00\\xa4\\x00\\xa2\\x00\\xa0\\x00\\x9e\\x00g\\x00@\\x00?\\x00>\\x003\\x002\\x001\\x000\\xc01\\xc0-\\xc0)\\xc0%\\xc0\\x0e\\xc0\"]\n",
      "Bad pipe message: %s [b'\\x9c\\x00<']\n",
      "Bad pipe message: %s [b\"k\\x97\\xaa\\xd5\\x89\\x8a\\x1d\\xaeP\\xf3\\x8dV\\x0cs\\x90q\\xb7\\xdf\\x00\\x00\\xf4\\xc00\\xc0,\\xc0(\\xc0$\\xc0\\x14\\xc0\\n\\x00\\xa5\\x00\\xa3\\x00\\xa1\\x00\\x9f\\x00k\\x00j\\x00i\\x00h\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\\x19\\x00\\xa7\\x00m\\x00:\\x00\\x89\\xc02\\xc0.\\xc0*\\xc0&\\xc0\\x0f\\xc0\\x05\\x00\\x9d\\x00=\\x005\\x00\\x84\\xc0/\\xc0+\\xc0'\\xc0#\\xc0\\x13\\xc0\\t\\x00\\xa4\\x00\\xa2\\x00\\xa0\\x00\\x9e\\x00g\\x00@\\x00?\\x00>\\x003\\x002\\x001\\x000\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00E\\x00D\\x00C\\x00B\\xc0\\x18\\x00\\xa6\\x00l\\x004\\x00\\x9b\\x00F\\xc01\\xc0-\\xc0)\\xc0%\\xc0\\x0e\\xc0\\x04\\x00\\x9c\\x00<\\x00/\\x00\\x96\\x00A\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x16\\x00\\x18\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\xc0\\x12\\xc0\\x08\\x00\\x16\\x00\\x13\\x00\\x10\\x00\\r\\xc0\\x17\\x00\\x1b\\xc0\\r\\xc0\\x03\\x00\\n\\x00\\x15\\x00\\x12\\x00\\x0f\\x00\\x0c\\x00\\x1a\\x00\\t\\x00\\x14\\x00\\x11\\x00\\x19\\x00\\x08\\x00\\x06\\x00\\x17\\x00\\x03\\xc0\\x10\\xc0\", b'\\x15\\xc0\\x0b\\xc0\\x01']\n"
     ]
    }
   ],
   "source": [
    "for flag in [\"train\",\"valid\",\"test\"]:\n",
    "    import random\n",
    "    import numpy as np\n",
    "    data = pickle.load(open(rf\"/home/xuyang1/PatchTST/1min_data/preprocessed_dataset_{flag}.pkl\", \"rb\"))\n",
    "    def pick_csi500(data, csi500_df):\n",
    "        instrument_id = data.index.get_level_values(\"instrument\").to_list()[0]\n",
    "        if instrument_id not in csi500_df.index:\n",
    "            return pd.DataFrame(columns=data.columns)\n",
    "        else:\n",
    "            return_df = None\n",
    "            csi500_time_period = csi500_df.loc[[instrument_id]]\n",
    "            picked_data = []\n",
    "            for _, row in csi500_time_period.iterrows():\n",
    "                pickled_df = data.loc[instrument_id].loc[row[\"start_time\"]:row[\"end_time\"]]\n",
    "                if len(pickled_df) != 0:\n",
    "                    picked_data.append(pickled_df)\n",
    "            if len(picked_data) == 0:\n",
    "                return pd.DataFrame(columns=data.columns)\n",
    "            if len(picked_data) == 1:\n",
    "                return_df = picked_data[0]\n",
    "            else:\n",
    "                picked_data = pd.concat(picked_data, axis=0)\n",
    "                return_df = picked_data\n",
    "            \n",
    "            num_groups = len(return_df) // 240  \n",
    "    \n",
    "            # 随机选择五分之一的组数  \n",
    "            selected_groups = np.random.choice(range(num_groups), num_groups // 5 + 1, replace=False)\n",
    "            \n",
    "            # 从原始DataFrame中提取所选组的样本，使用列表推导式避免for循环  \n",
    "            selected_samples = [return_df.iloc[group * 240:(group * 240) + 240] for group in selected_groups]  \n",
    "            \n",
    "            # 将选取的样本组合成一个新的DataFrame  \n",
    "            return_df = pd.concat(selected_samples, axis=0) \n",
    "            return return_df\n",
    "\n",
    "    # import random\n",
    "    # start = random.randint(0, 10000000)\n",
    "\n",
    "    data_new = data.groupby(\"instrument\").parallel_apply(pick_csi500, csi500_df=csi500_instrument).sort_index()\n",
    "    data_new.index.names = [\"instrument\", \"datetime\"]\n",
    "    print(flag, data_new.shape, data.shape)\n",
    "    data_new.to_pickle(rf\"/home/xuyang1/PatchTST/PatchTST_supervised/dataset/preprocessed_dataset_{flag}.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "display(data_new.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "patchtst",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3e1fed312b402d124a26fd1f35e64e518612f0eec7447ca06c3f2decdc2741cd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
